{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.io.json import json_normalize\n",
    "import json\n",
    "from textblob import TextBlob\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import text_processors\n",
    "from progressbar import ProgressBar\n",
    "import data_grab\n",
    "from time import time\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import pymc3 as pm \n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_metric(numpy_array_predictions, numpy_array_actual_values):\n",
    "    return metrics.weighted_rmsle(numpy_array_predictions, numpy_array_actual_values,\n",
    "            weights=metrics.KEEPING_IT_CLEAN_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_scoring(X, y, pipeline):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    s1 = pipeline.fit(X_train, y_train['score_lvl_1']).predict(X_test)\n",
    "    s2 = pipeline.fit(X_train, y_train['score_lvl_2']).predict(X_test)\n",
    "    s3 = pipeline.fit(X_train, y_train['score_lvl_3']).predict(X_test)\n",
    "    results = np.dstack((s1, s2, s3))\n",
    "    score = contest_metric(np.round(results[0]), np.array(y_test))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def raw_scoring(p1, p2, p3, ytrue):\n",
    "    '''since cross_val_score doesn't allow you to round the results beforehand. also for pymc3 and other non-sklearn models'''\n",
    "    score1 = accuracy_score(ytrue['score_lvl_1'], np.round(p1))\n",
    "    print(\"Level 1 accuracy score of {}\".format(score1))\n",
    "    score2 = accuracy_score(ytrue['score_lvl_2'], np.round(p2))\n",
    "    print(\"Level 2 accuracy score of {}\".format(score2))\n",
    "    score3 = accuracy_score(ytrue['score_lvl_3'], np.round(p3))\n",
    "    print(\"Level 3 accuracy score of {}\".format(score3))\n",
    "    \n",
    "    results = np.dstack((p1, p2, p3))[0]\n",
    "    score = contest_metric(np.round(results), np.array(ytrue))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    \n",
    "    rounded = np.clip(np.round(results), 0, np.inf)\n",
    "    compare = pd.concat([pd.DataFrame(np.concatenate((results, rounded), axis=1)), ytrue.reset_index(drop=True)], axis=1)\n",
    "    compare.columns = ['pred1','pred2','pred3','round1','round2','round3','true1','true2','true3']\n",
    "    compare['offset1'] = compare.round1-compare.true1\n",
    "    compare['offset2'] = compare.round2-compare.true2\n",
    "    compare['offset3'] = compare.round3-compare.true3\n",
    "        \n",
    "    return score1, score2, score3, score, compare.head(10)\n",
    "\n",
    "    \n",
    "def raw_fit(X, y, pipeline):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    p1 = pipeline.fit(xtrain, ytrain['score_lvl_1']).predict(xtest)\n",
    "    p2 = pipeline.fit(xtrain, ytrain['score_lvl_2']).predict(xtest)\n",
    "    p3 = pipeline.fit(xtrain, ytrain['score_lvl_3']).predict(xtest)\n",
    "        \n",
    "    return p1, p2, p3, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_model(X, y, pipeline):\n",
    "    scores = cross_val_score(pipeline, X, y, cv=3, n_jobs=1, verbose=1)\n",
    "    mean_score = np.mean(scores)\n",
    "    std_dev_score = np.std(scores)\n",
    "    print(\"CV score of {} +/- {}\".format(mean_score, std_dev_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    features = df.drop(['score_lvl_1', 'score_lvl_2', 'score_lvl_3'], axis=1)\n",
    "    response = df[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']].astype(np.int8)\n",
    "    \n",
    "    return features, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def multi_feature_test(X, y, pipeline, feature_list):\n",
    "    t0 = time()\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n",
    "     \n",
    "    combo_list = []\n",
    "    for num in range(1, len(feature_list)+1):\n",
    "        combo_list.extend([list(i) for i in combinations(feature_list, num)])\n",
    "    \n",
    "    temp_dict = {}\n",
    "    for features in combo_list:\n",
    "        p1 = pipeline.fit(xtrain, ytrain['score_lvl_1']).predict(xtest)\n",
    "#         print(\"Level 1 accuracy score of {} for {}\".format(accuracy_score(ytest['score_lvl_1'], np.round(p1)), features))\n",
    "        temp_dict.update({tuple(features): accuracy_score(ytest['score_lvl_1'], np.round(p1))})\n",
    "    print(\"{} seconds elapsed\".format(time()-t0))\n",
    "    return temp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "\n",
    "class TextStats(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from each document for DictVectorizer\"\"\"\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        return [{'length': len(text),\n",
    "                 'num_sentences': text.count('.')}\n",
    "                for text in posts]\n",
    "\n",
    "\n",
    "class SubjectBodyExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract the subject & body from a usenet post in a single pass.\n",
    "\n",
    "    Takes a sequence of strings and produces a dict of sequences.  Keys are\n",
    "    `subject` and `body`.\n",
    "    \"\"\"\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, posts):\n",
    "        features = np.recarray(shape=(len(posts),),\n",
    "                               dtype=[('subject', object), ('body', object)])\n",
    "        for i, text in enumerate(posts):\n",
    "            headers, _, bod = text.partition('\\n\\n')\n",
    "            bod = strip_newsgroup_footer(bod)\n",
    "            bod = strip_newsgroup_quoting(bod)\n",
    "            features['body'][i] = bod\n",
    "\n",
    "            prefix = 'Subject:'\n",
    "            sub = ''\n",
    "            for line in headers.split('\\n'):\n",
    "                if line.startswith(prefix):\n",
    "                    sub = line[len(prefix):]\n",
    "                    break\n",
    "            features['subject'][i] = sub\n",
    "\n",
    "        return features\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    # Extract the subject & body\n",
    "    ('subjectbody', SubjectBodyExtractor()),\n",
    "\n",
    "    # Use FeatureUnion to combine the features from subject and body\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "\n",
    "            # Pipeline for pulling features from the post's subject line\n",
    "            ('subject', Pipeline([\n",
    "                ('selector', ItemSelector(key='subject')),\n",
    "                ('tfidf', TfidfVectorizer(min_df=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for standard bag-of-words model for body\n",
    "            ('body_bow', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('tfidf', TfidfVectorizer()),\n",
    "                ('best', TruncatedSVD(n_components=50)),\n",
    "            ])),\n",
    "\n",
    "            # Pipeline for pulling ad hoc features from post's body\n",
    "            ('body_stats', Pipeline([\n",
    "                ('selector', ItemSelector(key='body')),\n",
    "                ('stats', TextStats()),  # returns a list of dicts\n",
    "                ('vect', DictVectorizer()),  # list of dicts -> feature matrix\n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "\n",
    "        # weight components in FeatureUnion\n",
    "        transformer_weights={\n",
    "            'subject': 0.8,\n",
    "            'body_bow': 0.5,\n",
    "            'body_stats': 1.0,\n",
    "        },\n",
    "    )),\n",
    "\n",
    "    # Use a SVC classifier on the combined features\n",
    "    ('svc', SVC(kernel='linear')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "pipe1 = ('tfidf',Pipeline([\n",
    "            ('low_var_removal', VarianceThreshold()),\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "pipe2 = ('other_features', Pipeline([\n",
    "            ('low_var_removal', VarianceThreshold()),\n",
    "            ('normalizer', Normalizer()),\n",
    "            ('scaler', StandardScaler()),\n",
    "        ])\n",
    "    )\n",
    "\n",
    "pipe1.fit(tfidf, y.score_lvl_1)\n",
    "pipe2.fit(others, y.score_lvl_1)\n",
    "\n",
    "estimator = Perceptron(n_jobs=-1, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('union', FeatureUnion(transformer_list=[pipe1, pipe2])),\n",
    "        ('estimator', estimator),\n",
    "        ])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "df = pd.read_pickle('pickle_jar/review_text_sentiment_hierarchical_df')\n",
    "# prep = pd.read_pickle('pickle_jar/preprocessed_review_text_hierarchical_df')\n",
    "# df = pd.concat([df, prep.preprocessed_review_text], axis=1)\n",
    "sim = pd.read_pickle('pickle_jar/similarity_vectors_df')\n",
    "# tfidf = joblib.load('pickle_jar/tfidf_preprocessed_ngram3_sublinear_1mil_hierarchical_dropna')\n",
    "# matrix = joblib.load('pickle_jar/similarity_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.previous_inspection_delta = df.previous_inspection_delta.fillna(0)\n",
    "df.previous_inspection_delta = df.previous_inspection_delta.dt.days.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (53 of 53) |#########################| Elapsed Time: 0:03:48 Time: 0:03:48\n"
     ]
    }
   ],
   "source": [
    "def get_out(x):\n",
    "    try:\n",
    "        return x[0]\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "topics = ['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "pbar = ProgressBar(maxval=len(topics)).start()\n",
    "for index, i in enumerate(topics):\n",
    "    sim[i] = sim[i].apply(get_out)\n",
    "    pbar.update(index)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.concat([df,sim[topics]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop('review_text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.drop(['review_date', 'user_id', 'restaurant_full_address', 'restaurant_name', 'inspection_date', 'inspection_id', 'inspection_date', 'sentiment', 'vader'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "labels ['restaurant_neighborhood_1' 'restaurant_neighborhood_2'\n 'restaurant_neighborhood_3' 'restaurant_category_1'\n 'restaurant_category_2' 'restaurant_category_3' 'restaurant_category_4'\n 'restaurant_category_5' 'restaurant_category_6' 'restaurant_category_7'] not contained in axis",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-218-8dd9516e932d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m  \u001b[0;34m'restaurant_category_5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m  \u001b[0;34m'restaurant_category_6'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m  'restaurant_category_7',], axis=1, inplace=True)\n\u001b[0m",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, level, inplace, errors)\u001b[0m\n\u001b[1;32m   1595\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1597\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1598\u001b[0m             \u001b[0mdropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1599\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   2568\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'ignore'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2570\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'labels %s not contained in axis'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2571\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: labels ['restaurant_neighborhood_1' 'restaurant_neighborhood_2'\n 'restaurant_neighborhood_3' 'restaurant_category_1'\n 'restaurant_category_2' 'restaurant_category_3' 'restaurant_category_4'\n 'restaurant_category_5' 'restaurant_category_6' 'restaurant_category_7'] not contained in axis"
     ]
    }
   ],
   "source": [
    "df.drop(['restaurant_neighborhood_1',\n",
    " 'restaurant_neighborhood_2',\n",
    " 'restaurant_neighborhood_3',\n",
    " 'restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data_grab.get_selects('train')\n",
    "dropped['restaurant_categories'] = train.restaurant_categories.apply(lambda x: sorted(x))\n",
    "dropped['restaurant_neighborhoods'] = train.restaurant_neighborhoods.apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = df.dropna(subset=['manager'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped['review_stars'] = dropped.review_stars.fillna(0).astype('category')\n",
    "dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer']] = dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = pd.read_pickle('pickle_jar/final_dropped')\n",
    "# dropped.to_pickle('pickle_jar/final_dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['restaurant_id',\n",
       " 'review_stars',\n",
       " 'review_votes_cool',\n",
       " 'review_votes_funny',\n",
       " 'review_votes_useful',\n",
       " 'user_average_stars',\n",
       " 'user_compliments_cool',\n",
       " 'user_compliments_cute',\n",
       " 'user_compliments_funny',\n",
       " 'user_compliments_hot',\n",
       " 'user_compliments_list',\n",
       " 'user_compliments_more',\n",
       " 'user_compliments_note',\n",
       " 'user_compliments_photos',\n",
       " 'user_compliments_plain',\n",
       " 'user_compliments_profile',\n",
       " 'user_compliments_writer',\n",
       " 'user_fans',\n",
       " 'user_name',\n",
       " 'user_review_count',\n",
       " 'user_votes_cool',\n",
       " 'user_votes_funny',\n",
       " 'user_votes_useful',\n",
       " 'user_yelping_since',\n",
       " 'restaurant_stars',\n",
       " 'restaurant_attributes_accepts_credit_cards',\n",
       " 'restaurant_attributes_ages_allowed',\n",
       " 'restaurant_attributes_alcohol',\n",
       " 'restaurant_attributes_attire',\n",
       " 'restaurant_attributes_byob',\n",
       " 'restaurant_attributes_byob_corkage',\n",
       " 'restaurant_attributes_by_appointment_only',\n",
       " 'restaurant_attributes_caters',\n",
       " 'restaurant_attributes_coat_check',\n",
       " 'restaurant_attributes_corkage',\n",
       " 'restaurant_attributes_delivery',\n",
       " 'restaurant_attributes_dietary_restrictions_dairy_free',\n",
       " 'restaurant_attributes_dietary_restrictions_gluten_free',\n",
       " 'restaurant_attributes_dietary_restrictions_halal',\n",
       " 'restaurant_attributes_dietary_restrictions_kosher',\n",
       " 'restaurant_attributes_dietary_restrictions_soy_free',\n",
       " 'restaurant_attributes_dietary_restrictions_vegan',\n",
       " 'restaurant_attributes_dietary_restrictions_vegetarian',\n",
       " 'restaurant_attributes_dogs_allowed',\n",
       " 'restaurant_attributes_drive_thr',\n",
       " 'restaurant_attributes_good_for_dancing',\n",
       " 'restaurant_attributes_good_for_groups',\n",
       " 'restaurant_attributes_good_for_breakfast',\n",
       " 'restaurant_attributes_good_for_brunch',\n",
       " 'restaurant_attributes_good_for_dessert',\n",
       " 'restaurant_attributes_good_for_dinner',\n",
       " 'restaurant_attributes_good_for_latenight',\n",
       " 'restaurant_attributes_good_for_lunch',\n",
       " 'restaurant_attributes_good_for_kids',\n",
       " 'restaurant_attributes_happy_hour',\n",
       " 'restaurant_attributes_has_tv',\n",
       " 'restaurant_attributes_noise_level',\n",
       " 'restaurant_attributes_open_24_hours',\n",
       " 'restaurant_attributes_order_at_counter',\n",
       " 'restaurant_attributes_outdoor_seating',\n",
       " 'restaurant_attributes_payment_types_amex',\n",
       " 'restaurant_attributes_payment_types_cash_only',\n",
       " 'restaurant_attributes_payment_types_discover',\n",
       " 'restaurant_attributes_payment_types_mastercard',\n",
       " 'restaurant_attributes_payment_types_visa',\n",
       " 'restaurant_attributes_price_range',\n",
       " 'restaurant_attributes_smoking',\n",
       " 'restaurant_attributes_take_out',\n",
       " 'restaurant_attributes_takes_reservations',\n",
       " 'restaurant_attributes_waiter_service',\n",
       " 'restaurant_attributes_wheelchair_accessible',\n",
       " 'restaurant_attributes_wifi',\n",
       " 'restaurant_city',\n",
       " 'restaurant_hours_friday_close',\n",
       " 'restaurant_hours_friday_open',\n",
       " 'restaurant_hours_monday_close',\n",
       " 'restaurant_hours_monday_open',\n",
       " 'restaurant_hours_saturday_close',\n",
       " 'restaurant_hours_saturday_open',\n",
       " 'restaurant_hours_sunday_close',\n",
       " 'restaurant_hours_sunday_open',\n",
       " 'restaurant_hours_thursday_close',\n",
       " 'restaurant_hours_thursday_open',\n",
       " 'restaurant_hours_tuesday_close',\n",
       " 'restaurant_hours_tuesday_open',\n",
       " 'restaurant_hours_wednesday_close',\n",
       " 'restaurant_hours_wednesday_open',\n",
       " 'restaurant_latitude',\n",
       " 'restaurant_longitude',\n",
       " 'restaurant_open',\n",
       " 'restaurant_review_count',\n",
       " 'checkin_counts',\n",
       " 'review_year',\n",
       " 'review_month',\n",
       " 'review_day',\n",
       " 'review_dayofweek',\n",
       " 'review_quarter',\n",
       " 'review_dayofyear',\n",
       " 'user_most_recent_elite_year',\n",
       " 'restaurant_ambience',\n",
       " 'restaurant_music',\n",
       " 'restaurant_parking',\n",
       " 'restaurant_street',\n",
       " 'restaurant_zipcode',\n",
       " 'score_lvl_1',\n",
       " 'score_lvl_2',\n",
       " 'score_lvl_3',\n",
       " 'review_delta',\n",
       " 'previous_inspection_delta',\n",
       " 'inspection_year',\n",
       " 'inspection_month',\n",
       " 'inspection_day',\n",
       " 'inspection_dayofweek',\n",
       " 'inspection_quarter',\n",
       " 'inspection_dayofyear',\n",
       " 'polarity',\n",
       " 'subjectivity',\n",
       " 'neg',\n",
       " 'neu',\n",
       " 'pos',\n",
       " 'compound',\n",
       " 'manager',\n",
       " 'supervisor',\n",
       " 'training',\n",
       " 'safety',\n",
       " 'disease',\n",
       " 'ill',\n",
       " 'sick',\n",
       " 'poisoning',\n",
       " 'hygiene',\n",
       " 'raw',\n",
       " 'undercooked',\n",
       " 'cold',\n",
       " 'clean',\n",
       " 'sanitary',\n",
       " 'wash',\n",
       " 'jaundice',\n",
       " 'yellow',\n",
       " 'hazard',\n",
       " 'inspection',\n",
       " 'violation',\n",
       " 'gloves',\n",
       " 'hairnet',\n",
       " 'nails',\n",
       " 'jewelry',\n",
       " 'sneeze',\n",
       " 'cough',\n",
       " 'runny',\n",
       " 'illegal',\n",
       " 'rotten',\n",
       " 'dirty',\n",
       " 'mouse',\n",
       " 'cockroach',\n",
       " 'contaminated',\n",
       " 'gross',\n",
       " 'disgusting',\n",
       " 'stink',\n",
       " 'old',\n",
       " 'parasite',\n",
       " 'reheat',\n",
       " 'frozen',\n",
       " 'broken',\n",
       " 'drip',\n",
       " 'bathroom',\n",
       " 'toilet',\n",
       " 'leak',\n",
       " 'trash',\n",
       " 'dark',\n",
       " 'lights',\n",
       " 'dust',\n",
       " 'puddle',\n",
       " 'pesticide',\n",
       " 'bugs',\n",
       " 'mold',\n",
       " 'restaurant_categories',\n",
       " 'restaurant_neighborhoods']"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1923536, 176)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17971,)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.restaurant_attributes_ages_allowed.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NaN        1905565\n",
       "21plus       17606\n",
       "allages        365\n",
       "dtype: int64"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.restaurant_attributes_ages_allowed.value_counts(dropna=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.restaurant_attributes_ages_allowed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for i in ['restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7']:\n",
    "    cats.extend(train[i].unique().tolist())\n",
    "cats = set(cats)\n",
    "cats.remove(np.nan)\n",
    "cats = sorted(cats)\n",
    "\n",
    "def proper_array(x, backfill_size=7):\n",
    "    encoder_prep = lambda x: cats.index(x)\n",
    "    temp = map(encoder_prep, x)\n",
    "    zeros = np.zeros(backfill_size, dtype='int')\n",
    "    zeros[:len(temp)] = temp\n",
    "    return zeros\n",
    "\n",
    "t = dropped.restaurant_categories.apply(proper_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11,  34,  63, ...,   0,   0,   0],\n",
       "       [ 11,  34,  63, ...,   0,   0,   0],\n",
       "       [ 11,  34,  63, ...,   0,   0,   0],\n",
       "       ..., \n",
       "       [ 27,  79, 129, ...,   0,   0,   0],\n",
       "       [ 27,  79, 129, ...,   0,   0,   0],\n",
       "       [ 27,  79, 129, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "e = enc.fit_transform(np.vstack(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_label = LabelEncoder()\n",
    "el = enc_label.fit_transform(np.vstack(t)[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "# others = df[['preprocessed_review_text', 'review_stars', 'review_delta', 'previous_inspection_delta', 'polarity', 'subjectivity', 'neg', 'pos', 'neu', 'compound', 'score_lvl_1', 'score_lvl_2', 'score_lvl_3']]\n",
    "# # drop nan rows according to review_text column to match with dropna-tfidf. fill review_star still existing nan's with 0 and then drop the review_text column completely\n",
    "# others.preprocessed_review_text = others.preprocessed_review_text.replace('', np.nan)\n",
    "# others = others.dropna(subset=['preprocessed_review_text']).fillna(0).drop('preprocessed_review_text', axis=1)\n",
    "# X = hstack([tfidf, others])\n",
    "# y = others[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# topics = ['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "\n",
    "\n",
    "# test = df[['review_delta', 'previous_inspection_delta', 'score_lvl_1', 'score_lvl_2', 'score_lvl_3']+topics]\n",
    "# test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "# def get_out(x):\n",
    "#     try:\n",
    "#         return x[0]\n",
    "#     except:\n",
    "#         return x\n",
    "\n",
    "# topics = ['supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "# matrix = np.vstack(test.manager.apply(lambda x: x[0:2]))\n",
    "# pbar = ProgressBar(maxval=len(topics)).start()\n",
    "# for index, i in enumerate(topics):\n",
    "#     t = np.vstack(test[i].apply(lambda x: x[0:2]))\n",
    "#     matrix = np.concatenate((matrix, t), axis=1)\n",
    "#     pbar.update(index)\n",
    "# pbar.finish()\n",
    "# matrix = csr_matrix(matrix)\n",
    "# X = hstack([matrix, test[['review_delta', 'previous_inspection_delta']]])\n",
    "# y = test[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix, hstack\n",
    "# topics = ['supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "# matrix = csr_matrix(np.vstack(test.manager.apply(lambda x: x[0:5])))\n",
    "# pbar = ProgressBar(maxval=len(topics)).start()\n",
    "# for index,i in enumerate(topics):\n",
    "#     t = np.vstack(test[i].apply(lambda x: x[0:5]))\n",
    "#     matrix = hstack([matrix, t])\n",
    "#     pbar.update(index)\n",
    "# pbar.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = joblib.load('pickle_jar/similarity_matrix5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1923536, 265)\n",
      "(1923536, 58)\n"
     ]
    }
   ],
   "source": [
    "print matrix.shape\n",
    "print test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from scipy.sparse import csr_matrix, hstack\n",
    "# X = hstack([matrix, test[['review_delta', 'previous_inspection_delta']]])\n",
    "# y = test[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# del matrix\n",
    "# del df\n",
    "# del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bins(df, bin_size=10):\n",
    "    # time delta bins\n",
    "    tdmax = df.review_delta.max()\n",
    "    tdmin = df.review_delta.min()\n",
    "    df['review_delta_bin'] = pd.cut(df[\"review_delta\"], np.arange(tdmin, tdmax, bin_size))\n",
    "    df['review_delta_bin_codes'] = df.review_delta_bin.astype('category').cat.codes\n",
    "    tdmax = df.previous_inspection_delta.max()\n",
    "    tdmin = df.previous_inspection_delta.min()\n",
    "    df['previous_inspection_delta_bin'] = pd.cut(df[\"previous_inspection_delta\"], np.arange(tdmin-1, tdmax, bin_size))\n",
    "    df['previous_inspection_delta_bin_codes'] = df.previous_inspection_delta_bin.astype('category').cat.codes\n",
    "    return df\n",
    "df = make_bins(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4071065, 185)\n",
      "(1923536, 55)\n",
      "(1923536, 3)\n"
     ]
    }
   ],
   "source": [
    "scores = ['score_lvl_1', 'score_lvl_2', 'score_lvl_3']\n",
    "# model_features = ['review_stars', 'review_delta', 'previous_inspection_delta','review_delta_bin', 'previous_inspection_delta_bin', 'polarity', 'subjectivity', 'neg', 'pos', 'neu', 'compound']\n",
    "# model_features = ['review_stars', 'review_delta', 'previous_inspection_delta', 'polarity', 'subjectivity', 'neg', 'pos', 'neu', 'compound']\n",
    "model_features = ['review_delta', 'previous_inspection_delta']\n",
    "\n",
    "X, y = extract_features(test[model_features + topics + scores].dropna())\n",
    "\n",
    "print df.shape\n",
    "print X.shape\n",
    "print y.shape\n",
    "# review_stars doesnt exist for every observation so reducing size even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import RFECV\n",
    "# rfecv = RFECV(estimator=SGDClassifier(n_jobs=-1), scoring=mean_squared_error)\n",
    "# rfecv.fit(X, y.score_lvl_1)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# from sklearn.svm import LinearSVC\n",
    "# X_new = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit_transform(X, y.score_lvl_1)\n",
    "\n",
    "# from sklearn.feature_selection import RFE\n",
    "# rfe = RFE(estimator=LinearRegression(), n_features_to_select=3, step=1)\n",
    "# rfe.fit(X, y.score_lvl_1)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "test = SelectFwe(f_classif).fit_transform(X, y.score_lvl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_new = SelectKBest(f_classif, k=5).fit_transform(X, y.score_lvl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 7, 6, 3, 4, 1, 1, 1, 5])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1923536x267 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 504667441 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 accuracy score of 0.140464228379\n",
      "Level 2 accuracy score of 0.693059032948\n",
      "Level 3 accuracy score of 0.574184210745\n",
      "Contest score of 1.43479163855\n",
      "554.074513912 seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "# set classifiers to test\n",
    "estimator = LinearRegression()\n",
    "# estimator = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = SGDClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = Perceptron(n_jobs=-1, random_state=42)  # gets some nuances\n",
    "# estimator = SGDRegressor() # gets some nuances\n",
    "# estimator = KNeighborsClassifier()\n",
    "# estimator = KNeighborsRegressor()  # gets some nuances\n",
    "# estimator = DecisionTreeClassifier()\n",
    "# estimator = DecisionTreeRegressor()\n",
    "# estimator = GaussianNB()\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('low_var_removal', VarianceThreshold()),\n",
    "        ('normalizer', Normalizer()),\n",
    "#         ('normalizer', Normalizer(norm='l2')), #  for text classification and clustering\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('scaler', StandardScaler(with_mean=False)), #  for sparse matrix\n",
    "        ('clf', estimator),\n",
    "])\n",
    "\n",
    "p1,p2,p3,ytest = raw_fit(X, y, pipeline)\n",
    "raw_scoring(p1,p2,p3,ytest)\n",
    "\n",
    "\n",
    "print(\"{} seconds elapsed\".format(time()-t0))\n",
    "\n",
    "# first representation of manager and mold plus deltas for SGDClassifier\n",
    "# Level 1 accuracy score of 0.160254863959\n",
    "# Level 2 accuracy score of 0.693050714933\n",
    "# Level 3 accuracy score of 0.574146779681\n",
    "# Contest score of 1.52839121903\n",
    "\n",
    "# sparse matrix of just manager no-mean-false and the detla for sdg\n",
    "# Level 1 accuracy score of 0.136340572778\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.41908388602\n",
    "\n",
    "# first representation of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.0958193660009\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.2018434011\n",
    "\n",
    "# first two representations of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.134818376157\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.73133292154\n",
    "\n",
    "# first five representations of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.140464228379\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.43479163855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1: 0.22128560699\n",
      "level 2: 0.692799495547\n",
      "level 3: 0.573199692093\n"
     ]
    }
   ],
   "source": [
    "# baseline scores if guessing zero\n",
    "guess = 0\n",
    "for index, score in enumerate(scores):\n",
    "    print(\"level {}: {}\".format(index+1, y[score].value_counts(normalize=True)[guess]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>score_lvl_1</td>   <th>  R-squared:         </th>  <td>   0.011</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.011</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   2576.</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 11 Jul 2015</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>13:24:09</td>     <th>  Log-Likelihood:    </th> <td>-5.7435e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>1925254</td>     <th>  AIC:               </th>  <td>1.149e+07</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>1925245</td>     <th>  BIC:               </th>  <td>1.149e+07</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     8</td>      <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>     \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                 <td>    3.9054</td> <td>    0.447</td> <td>    8.737</td> <td> 0.000</td> <td>    3.029     4.782</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>review_delta</th>              <td>    0.0001</td> <td> 5.08e-06</td> <td>   26.612</td> <td> 0.000</td> <td>    0.000     0.000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>previous_inspection_delta</th> <td>   -0.0030</td> <td> 2.12e-05</td> <td> -141.223</td> <td> 0.000</td> <td>   -0.003    -0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>polarity</th>                  <td>   -0.0294</td> <td>    0.022</td> <td>   -1.321</td> <td> 0.187</td> <td>   -0.073     0.014</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subjectivity</th>              <td>    0.0467</td> <td>    0.025</td> <td>    1.886</td> <td> 0.059</td> <td>   -0.002     0.095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>neg</th>                       <td>    0.9265</td> <td>    0.453</td> <td>    2.046</td> <td> 0.041</td> <td>    0.039     1.814</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pos</th>                       <td>    0.7716</td> <td>    0.449</td> <td>    1.719</td> <td> 0.086</td> <td>   -0.108     1.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>neu</th>                       <td>    0.7430</td> <td>    0.447</td> <td>    1.661</td> <td> 0.097</td> <td>   -0.134     1.620</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>compound</th>                  <td>   -0.0587</td> <td>    0.009</td> <td>   -6.333</td> <td> 0.000</td> <td>   -0.077    -0.041</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1120039.389</td> <th>  Durbin-Watson:     </th>   <td>   0.017</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>   <td> 0.000</td>    <th>  Jarque-Bera (JB):  </th> <td>13578799.817</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>            <td> 2.602</td>    <th>  Prob(JB):          </th>   <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>        <td>14.925</td>    <th>  Cond. No.          </th>   <td>2.96e+05</td>  \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:            score_lvl_1   R-squared:                       0.011\n",
       "Model:                            OLS   Adj. R-squared:                  0.011\n",
       "Method:                 Least Squares   F-statistic:                     2576.\n",
       "Date:                Sat, 11 Jul 2015   Prob (F-statistic):               0.00\n",
       "Time:                        13:24:09   Log-Likelihood:            -5.7435e+06\n",
       "No. Observations:             1925254   AIC:                         1.149e+07\n",
       "Df Residuals:                 1925245   BIC:                         1.149e+07\n",
       "Df Model:                           8                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "=============================================================================================\n",
       "                                coef    std err          t      P>|t|      [95.0% Conf. Int.]\n",
       "---------------------------------------------------------------------------------------------\n",
       "Intercept                     3.9054      0.447      8.737      0.000         3.029     4.782\n",
       "review_delta                  0.0001   5.08e-06     26.612      0.000         0.000     0.000\n",
       "previous_inspection_delta    -0.0030   2.12e-05   -141.223      0.000        -0.003    -0.003\n",
       "polarity                     -0.0294      0.022     -1.321      0.187        -0.073     0.014\n",
       "subjectivity                  0.0467      0.025      1.886      0.059        -0.002     0.095\n",
       "neg                           0.9265      0.453      2.046      0.041         0.039     1.814\n",
       "pos                           0.7716      0.449      1.719      0.086        -0.108     1.651\n",
       "neu                           0.7430      0.447      1.661      0.097        -0.134     1.620\n",
       "compound                     -0.0587      0.009     -6.333      0.000        -0.077    -0.041\n",
       "==============================================================================\n",
       "Omnibus:                  1120039.389   Durbin-Watson:                   0.017\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):         13578799.817\n",
       "Skew:                           2.602   Prob(JB):                         0.00\n",
       "Kurtosis:                      14.925   Cond. No.                     2.96e+05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 2.96e+05. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "model = smf.ols(formula='score_lvl_1 ~'+'+'.join(model_features), data=df[model_features + scores].dropna()).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.559156\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Poisson Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>    <td>score_lvl_1</td>   <th>  No. Observations:  </th>   <td>1781150</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>              <td>Poisson</td>     <th>  Df Residuals:      </th>   <td>1781141</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>   <td>     8</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Sat, 11 Jul 2015</td> <th>  Pseudo R-squ.:     </th>  <td>0.008638</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>16:24:54</td>     <th>  Log-Likelihood:    </th> <td>-6.3394e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td>-6.3946e+06</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th>   <td> 0.000</td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "              <td></td>                 <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>review_stars</th>              <td>   -0.0185</td> <td>    0.000</td> <td>  -48.993</td> <td> 0.000</td> <td>   -0.019    -0.018</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>review_delta</th>              <td> 2.759e-05</td> <td> 5.17e-07</td> <td>   53.338</td> <td> 0.000</td> <td> 2.66e-05  2.86e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>previous_inspection_delta</th> <td>   -0.0008</td> <td> 2.54e-06</td> <td> -309.734</td> <td> 0.000</td> <td>   -0.001    -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>polarity</th>                  <td>    0.0180</td> <td>    0.003</td> <td>    6.752</td> <td> 0.000</td> <td>    0.013     0.023</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>subjectivity</th>              <td>   -0.0035</td> <td>    0.003</td> <td>   -1.061</td> <td> 0.289</td> <td>   -0.010     0.003</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>neg</th>                       <td>    1.5881</td> <td>    0.009</td> <td>  173.016</td> <td> 0.000</td> <td>    1.570     1.606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pos</th>                       <td>    1.6154</td> <td>    0.005</td> <td>  297.329</td> <td> 0.000</td> <td>    1.605     1.626</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>neu</th>                       <td>    1.6129</td> <td>    0.002</td> <td>  727.107</td> <td> 0.000</td> <td>    1.609     1.617</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>compound</th>                  <td>   -0.0023</td> <td>    0.001</td> <td>   -2.155</td> <td> 0.031</td> <td>   -0.004    -0.000</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                          Poisson Regression Results                          \n",
       "==============================================================================\n",
       "Dep. Variable:            score_lvl_1   No. Observations:              1781150\n",
       "Model:                        Poisson   Df Residuals:                  1781141\n",
       "Method:                           MLE   Df Model:                            8\n",
       "Date:                Sat, 11 Jul 2015   Pseudo R-squ.:                0.008638\n",
       "Time:                        16:24:54   Log-Likelihood:            -6.3394e+06\n",
       "converged:                       True   LL-Null:                   -6.3946e+06\n",
       "                                        LLR p-value:                     0.000\n",
       "=============================================================================================\n",
       "                                coef    std err          z      P>|z|      [95.0% Conf. Int.]\n",
       "---------------------------------------------------------------------------------------------\n",
       "review_stars                 -0.0185      0.000    -48.993      0.000        -0.019    -0.018\n",
       "review_delta               2.759e-05   5.17e-07     53.338      0.000      2.66e-05  2.86e-05\n",
       "previous_inspection_delta    -0.0008   2.54e-06   -309.734      0.000        -0.001    -0.001\n",
       "polarity                      0.0180      0.003      6.752      0.000         0.013     0.023\n",
       "subjectivity                 -0.0035      0.003     -1.061      0.289        -0.010     0.003\n",
       "neg                           1.5881      0.009    173.016      0.000         1.570     1.606\n",
       "pos                           1.6154      0.005    297.329      0.000         1.605     1.626\n",
       "neu                           1.6129      0.002    727.107      0.000         1.609     1.617\n",
       "compound                     -0.0023      0.001     -2.155      0.031        -0.004    -0.000\n",
       "=============================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from statsmodels.discrete.discrete_model import Poisson\n",
    "model = Poisson(endog=y.score_lvl_1, exog=df[model_features].dropna()).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>review_delta</th>\n",
       "      <td>19206.698621</td>\n",
       "      <td>1</td>\n",
       "      <td>841.975840</td>\n",
       "      <td>4.433674e-185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>previous_inspection_delta</th>\n",
       "      <td>463551.215022</td>\n",
       "      <td>1</td>\n",
       "      <td>20320.979218</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>polarity</th>\n",
       "      <td>11.553256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.506467</td>\n",
       "      <td>4.766723e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subjectivity</th>\n",
       "      <td>32.118953</td>\n",
       "      <td>1</td>\n",
       "      <td>1.408018</td>\n",
       "      <td>2.353858e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>90.939919</td>\n",
       "      <td>1</td>\n",
       "      <td>3.986589</td>\n",
       "      <td>4.586396e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>62.607017</td>\n",
       "      <td>1</td>\n",
       "      <td>2.744542</td>\n",
       "      <td>9.758718e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu</th>\n",
       "      <td>58.170630</td>\n",
       "      <td>1</td>\n",
       "      <td>2.550062</td>\n",
       "      <td>1.102901e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound</th>\n",
       "      <td>1005.620308</td>\n",
       "      <td>1</td>\n",
       "      <td>44.083995</td>\n",
       "      <td>3.146670e-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>43210335.339146</td>\n",
       "      <td>1894238</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    sum_sq       df             F  \\\n",
       "review_delta                  19206.698621        1    841.975840   \n",
       "previous_inspection_delta    463551.215022        1  20320.979218   \n",
       "polarity                         11.553256        1      0.506467   \n",
       "subjectivity                     32.118953        1      1.408018   \n",
       "neg                              90.939919        1      3.986589   \n",
       "pos                              62.607017        1      2.744542   \n",
       "neu                              58.170630        1      2.550062   \n",
       "compound                       1005.620308        1     44.083995   \n",
       "Residual                   43210335.339146  1894238           NaN   \n",
       "\n",
       "                                  PR(>F)  \n",
       "review_delta               4.433674e-185  \n",
       "previous_inspection_delta   0.000000e+00  \n",
       "polarity                    4.766723e-01  \n",
       "subjectivity                2.353858e-01  \n",
       "neg                         4.586396e-02  \n",
       "pos                         9.758718e-02  \n",
       "neu                         1.102901e-01  \n",
       "compound                    3.146670e-11  \n",
       "Residual                             NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "sm.stats.anova_lm(model, typ=2) # Type 2 ANOVA DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept      -3.550546\n",
      "review_delta    0.000927\n",
      "polarity        0.294398\n",
      "subjectivity   -0.430076\n",
      "neg             8.069998\n",
      "pos             8.031545\n",
      "neu             7.440557\n",
      "compound       -0.328166\n",
      "dtype: float64\n",
      "Group effect test [[ 242.21228155]]\n",
      "Covariate effect test [[ 7.02122616]]\n"
     ]
    }
   ],
   "source": [
    "print model.params\n",
    "A=np.identity(len(model.params)) # identity matrix with size = number of params\n",
    "GroupTest=A[1:3,:] # for the categorical var., keep the corresponding rows of A\n",
    "CovTest=A[3,:] # row for the continuous var.\n",
    "print \"Group effect test\",model.f_test(GroupTest).fvalue\n",
    "print \"Covariate effect test\",model.f_test(CovTest).fvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc3 import Model, Normal, HalfNormal\n",
    "basic_model = Model()\n",
    "\n",
    "with basic_model:\n",
    "\n",
    "    # Priors for unknown model parameters\n",
    "    alpha = Normal('alpha', mu=0, sd=10)\n",
    "    beta = Normal('beta', mu=0, sd=10, shape=2)\n",
    "    sigma = HalfNormal('sigma', sd=1)\n",
    "\n",
    "    # Expected value of outcome\n",
    "    mu = alpha + beta[0]*X.review_delta + beta[1]*X.neg\n",
    "\n",
    "    # Likelihood (sampling distribution) of observations\n",
    "    Y_obs = Normal('Y_obs', mu=mu, sd=sigma, observed=y.score_lvl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': array(4.20236359846617), 'beta': array([  1.16082103e-04,   4.84806122e-01]), 'sigma_log': array(1.5699115275497426)}\n"
     ]
    }
   ],
   "source": [
    "from pymc3 import find_MAP\n",
    "\n",
    "from scipy import optimize\n",
    "\n",
    "map_estimate = find_MAP(model=basic_model, fmin=optimize.fmin_powell)\n",
    "\n",
    "print(map_estimate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pymc3.glm import glm\n",
    "\n",
    "with Model() as model_glm:\n",
    "    glm('score_lvl_1 ~'+'+'.join(model_features), df[model_features + scores].dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
