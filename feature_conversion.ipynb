{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import text_processors\n",
    "from progressbar import ProgressBar\n",
    "import data_grab\n",
    "from time import time\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_metric(numpy_array_predictions, numpy_array_actual_values):\n",
    "    return metrics.weighted_rmsle(numpy_array_predictions, numpy_array_actual_values,\n",
    "            weights=metrics.KEEPING_IT_CLEAN_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_scoring(X, y, pipeline):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    s1 = pipeline.fit(X_train, y_train['score_lvl_1']).predict(X_test)\n",
    "    s2 = pipeline.fit(X_train, y_train['score_lvl_2']).predict(X_test)\n",
    "    s3 = pipeline.fit(X_train, y_train['score_lvl_3']).predict(X_test)\n",
    "    results = np.dstack((s1, s2, s3))\n",
    "    score = contest_metric(np.round(results[0]), np.array(y_test))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def raw_scoring(p1, p2, p3, ytrue):\n",
    "    '''since cross_val_score doesn't allow you to round the results beforehand. also for pymc3 and other non-sklearn models'''\n",
    "    score1 = accuracy_score(ytrue['score_lvl_1'], np.clip(np.round(p1), 0, np.inf))\n",
    "    print(\"Level 1 accuracy score of {}\".format(score1))\n",
    "    score2 = accuracy_score(ytrue['score_lvl_2'],np.clip(np.round(p2), 0, np.inf))\n",
    "    print(\"Level 2 accuracy score of {}\".format(score2))\n",
    "    score3 = accuracy_score(ytrue['score_lvl_3'], np.clip(np.round(p3), 0, np.inf))\n",
    "    print(\"Level 3 accuracy score of {}\".format(score3))\n",
    "    \n",
    "    results = np.dstack((p1, p2, p3))[0]\n",
    "    rounded = np.clip(np.round(results), 0, np.inf)\n",
    "    score = contest_metric(rounded, np.array(ytrue))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    \n",
    "    compare = pd.concat([pd.DataFrame(np.concatenate((results, rounded), axis=1)), ytrue.reset_index(drop=True)], axis=1)\n",
    "    compare.columns = ['pred1','pred2','pred3','round1','round2','round3','true1','true2','true3']\n",
    "    compare['offset1'] = compare.round1-compare.true1\n",
    "    compare['offset2'] = compare.round2-compare.true2\n",
    "    compare['offset3'] = compare.round3-compare.true3\n",
    "        \n",
    "    return score1, score2, score3, score, compare.head(10)\n",
    "\n",
    "    \n",
    "def raw_fit(X, y, pipeline):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    p1 = pipeline.fit(xtrain, ytrain['score_lvl_1']).predict(xtest)\n",
    "    p2 = pipeline.fit(xtrain, ytrain['score_lvl_2']).predict(xtest)\n",
    "    p3 = pipeline.fit(xtrain, ytrain['score_lvl_3']).predict(xtest)\n",
    "        \n",
    "    return p1, p2, p3, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    features = df.drop(['score_lvl_1', 'score_lvl_2', 'score_lvl_3'], axis=1)\n",
    "    response = df[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']].astype(np.int8)\n",
    "    \n",
    "    return features, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pickle_jar/review_text_sentiment_flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant_id</th>\n",
       "      <th>inspection_date</th>\n",
       "      <th>inspection_id</th>\n",
       "      <th>review_text</th>\n",
       "      <th>score_lvl_1</th>\n",
       "      <th>score_lvl_2</th>\n",
       "      <th>score_lvl_3</th>\n",
       "      <th>preprocessed_review_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N6Ok7qOx</td>\n",
       "      <td>2007-11-08</td>\n",
       "      <td>23100</td>\n",
       "      <td>This is a pretty typical cafe.  The sandwiches...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this pretty typical cafe the sandwich wrap goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2012-08-02</td>\n",
       "      <td>27889</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2011-11-03</td>\n",
       "      <td>11070</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2010-04-08</td>\n",
       "      <td>8714</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2009-12-10</td>\n",
       "      <td>5843</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2011-11-28</td>\n",
       "      <td>29966</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2008-07-03</td>\n",
       "      <td>19729</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>29636</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>11606</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2011-04-29</td>\n",
       "      <td>14517</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2012-08-24</td>\n",
       "      <td>19218</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2010-12-20</td>\n",
       "      <td>9253</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2012-08-16</td>\n",
       "      <td>15854</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>p03824Om</td>\n",
       "      <td>2008-06-27</td>\n",
       "      <td>18395</td>\n",
       "      <td>This is the place I like to go for deli sandwi...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>this place i like go deli sandwich financial d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2008-09-11</td>\n",
       "      <td>5134</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2011-04-13</td>\n",
       "      <td>12823</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2012-08-28</td>\n",
       "      <td>26735</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2011-11-17</td>\n",
       "      <td>23744</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2010-12-08</td>\n",
       "      <td>30593</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2010-08-11</td>\n",
       "      <td>29103</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2012-04-26</td>\n",
       "      <td>3824</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2012-04-27</td>\n",
       "      <td>8070</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>2156</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2009-03-11</td>\n",
       "      <td>1223</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>7815</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>32234</td>\n",
       "      <td>Considering I've worked upstairs from this pla...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>considering i ve work upstairs place figure i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2007-12-13</td>\n",
       "      <td>25158</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2007-06-19</td>\n",
       "      <td>6165</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2008-04-23</td>\n",
       "      <td>17430</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8KoAk4E6</td>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>8922</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27058</th>\n",
       "      <td>B1oXNlEV</td>\n",
       "      <td>2014-08-12</td>\n",
       "      <td>28022</td>\n",
       "      <td>Chocolate, chocolate, chocolate!  This place d...</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>chocolate chocolate chocolate this place cockt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27059</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2011-10-13</td>\n",
       "      <td>26143</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27060</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2012-11-19</td>\n",
       "      <td>10317</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27061</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2015-01-12</td>\n",
       "      <td>25605</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27062</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2013-01-08</td>\n",
       "      <td>29292</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27063</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2011-03-31</td>\n",
       "      <td>3822</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27064</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2012-11-26</td>\n",
       "      <td>18338</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27065</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2011-03-29</td>\n",
       "      <td>18871</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27066</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2011-10-24</td>\n",
       "      <td>33029</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27067</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2014-04-25</td>\n",
       "      <td>21578</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27068</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>17903</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27069</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2015-01-06</td>\n",
       "      <td>22405</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27070</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2011-03-22</td>\n",
       "      <td>8989</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27071</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2012-06-22</td>\n",
       "      <td>19823</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27072</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2012-06-19</td>\n",
       "      <td>11862</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27073</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2013-11-19</td>\n",
       "      <td>33</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27074</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2013-04-22</td>\n",
       "      <td>9617</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27075</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2014-04-15</td>\n",
       "      <td>19748</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27076</th>\n",
       "      <td>6VOp5QEL</td>\n",
       "      <td>2013-12-30</td>\n",
       "      <td>27219</td>\n",
       "      <td>Fresh fish. Great prices. Friendly staff. \\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>fresh fish great price friendly staff i m exci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27077</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2013-01-11</td>\n",
       "      <td>30692</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27078</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2012-04-24</td>\n",
       "      <td>24766</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27079</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2013-09-17</td>\n",
       "      <td>21215</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27080</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2011-04-07</td>\n",
       "      <td>24133</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27081</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2011-08-12</td>\n",
       "      <td>14917</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27082</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2012-08-08</td>\n",
       "      <td>10872</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27083</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2013-09-10</td>\n",
       "      <td>25907</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27084</th>\n",
       "      <td>ZBEn9vOY</td>\n",
       "      <td>2015-03-06</td>\n",
       "      <td>4036</td>\n",
       "      <td>So it was fate that I happened to be checking ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>so fate i happen check twitter feed day find b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27085</th>\n",
       "      <td>qN3g1QOA</td>\n",
       "      <td>2014-04-03</td>\n",
       "      <td>27340</td>\n",
       "      <td>Mama's Place might be our new favorite local p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mama place might new favorite local place take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27086</th>\n",
       "      <td>qN3g1QOA</td>\n",
       "      <td>2014-11-20</td>\n",
       "      <td>288</td>\n",
       "      <td>Mama's Place might be our new favorite local p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mama place might new favorite local place take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27087</th>\n",
       "      <td>qN3g1QOA</td>\n",
       "      <td>2014-02-10</td>\n",
       "      <td>19649</td>\n",
       "      <td>Mama's Place might be our new favorite local p...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>mama place might new favorite local place take...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27088 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      restaurant_id inspection_date  inspection_id  \\\n",
       "0          N6Ok7qOx      2007-11-08          23100   \n",
       "1          p03824Om      2012-08-02          27889   \n",
       "2          p03824Om      2011-11-03          11070   \n",
       "3          p03824Om      2010-04-08           8714   \n",
       "4          p03824Om      2009-12-10           5843   \n",
       "5          p03824Om      2011-11-28          29966   \n",
       "6          p03824Om      2008-07-03          19729   \n",
       "7          p03824Om      2008-12-31          29636   \n",
       "8          p03824Om      2010-04-01          11606   \n",
       "9          p03824Om      2011-04-29          14517   \n",
       "10         p03824Om      2012-08-24          19218   \n",
       "11         p03824Om      2010-12-20           9253   \n",
       "12         p03824Om      2012-08-16          15854   \n",
       "13         p03824Om      2008-06-27          18395   \n",
       "14         8KoAk4E6      2008-09-11           5134   \n",
       "15         8KoAk4E6      2011-04-13          12823   \n",
       "16         8KoAk4E6      2012-08-28          26735   \n",
       "17         8KoAk4E6      2011-11-17          23744   \n",
       "18         8KoAk4E6      2010-12-08          30593   \n",
       "19         8KoAk4E6      2010-08-11          29103   \n",
       "20         8KoAk4E6      2012-04-26           3824   \n",
       "21         8KoAk4E6      2012-04-27           8070   \n",
       "22         8KoAk4E6      2011-12-01           2156   \n",
       "23         8KoAk4E6      2009-03-11           1223   \n",
       "24         8KoAk4E6      2012-12-30           7815   \n",
       "25         8KoAk4E6      2011-07-27          32234   \n",
       "26         8KoAk4E6      2007-12-13          25158   \n",
       "27         8KoAk4E6      2007-06-19           6165   \n",
       "28         8KoAk4E6      2008-04-23          17430   \n",
       "29         8KoAk4E6      2008-03-31           8922   \n",
       "...             ...             ...            ...   \n",
       "27058      B1oXNlEV      2014-08-12          28022   \n",
       "27059      6VOp5QEL      2011-10-13          26143   \n",
       "27060      6VOp5QEL      2012-11-19          10317   \n",
       "27061      6VOp5QEL      2015-01-12          25605   \n",
       "27062      6VOp5QEL      2013-01-08          29292   \n",
       "27063      6VOp5QEL      2011-03-31           3822   \n",
       "27064      6VOp5QEL      2012-11-26          18338   \n",
       "27065      6VOp5QEL      2011-03-29          18871   \n",
       "27066      6VOp5QEL      2011-10-24          33029   \n",
       "27067      6VOp5QEL      2014-04-25          21578   \n",
       "27068      6VOp5QEL      2012-12-30          17903   \n",
       "27069      6VOp5QEL      2015-01-06          22405   \n",
       "27070      6VOp5QEL      2011-03-22           8989   \n",
       "27071      6VOp5QEL      2012-06-22          19823   \n",
       "27072      6VOp5QEL      2012-06-19          11862   \n",
       "27073      6VOp5QEL      2013-11-19             33   \n",
       "27074      6VOp5QEL      2013-04-22           9617   \n",
       "27075      6VOp5QEL      2014-04-15          19748   \n",
       "27076      6VOp5QEL      2013-12-30          27219   \n",
       "27077      ZBEn9vOY      2013-01-11          30692   \n",
       "27078      ZBEn9vOY      2012-04-24          24766   \n",
       "27079      ZBEn9vOY      2013-09-17          21215   \n",
       "27080      ZBEn9vOY      2011-04-07          24133   \n",
       "27081      ZBEn9vOY      2011-08-12          14917   \n",
       "27082      ZBEn9vOY      2012-08-08          10872   \n",
       "27083      ZBEn9vOY      2013-09-10          25907   \n",
       "27084      ZBEn9vOY      2015-03-06           4036   \n",
       "27085      qN3g1QOA      2014-04-03          27340   \n",
       "27086      qN3g1QOA      2014-11-20            288   \n",
       "27087      qN3g1QOA      2014-02-10          19649   \n",
       "\n",
       "                                             review_text  score_lvl_1  \\\n",
       "0      This is a pretty typical cafe.  The sandwiches...            2   \n",
       "1      This is the place I like to go for deli sandwi...            7   \n",
       "2      This is the place I like to go for deli sandwi...            2   \n",
       "3      This is the place I like to go for deli sandwi...            7   \n",
       "4      This is the place I like to go for deli sandwi...            0   \n",
       "5      This is the place I like to go for deli sandwi...            1   \n",
       "6      This is the place I like to go for deli sandwi...            2   \n",
       "7      This is the place I like to go for deli sandwi...            2   \n",
       "8      This is the place I like to go for deli sandwi...            7   \n",
       "9      This is the place I like to go for deli sandwi...            0   \n",
       "10     This is the place I like to go for deli sandwi...            7   \n",
       "11     This is the place I like to go for deli sandwi...            1   \n",
       "12     This is the place I like to go for deli sandwi...            7   \n",
       "13     This is the place I like to go for deli sandwi...            2   \n",
       "14                                                                  5   \n",
       "15     Considering I've worked upstairs from this pla...            6   \n",
       "16     Considering I've worked upstairs from this pla...           17   \n",
       "17     Considering I've worked upstairs from this pla...            9   \n",
       "18     Considering I've worked upstairs from this pla...            8   \n",
       "19     Considering I've worked upstairs from this pla...            9   \n",
       "20     Considering I've worked upstairs from this pla...            0   \n",
       "21     Considering I've worked upstairs from this pla...           14   \n",
       "22     Considering I've worked upstairs from this pla...            9   \n",
       "23     Considering I've worked upstairs from this pla...            4   \n",
       "24     Considering I've worked upstairs from this pla...            0   \n",
       "25     Considering I've worked upstairs from this pla...            5   \n",
       "26                                                                  8   \n",
       "27                                                                  4   \n",
       "28                                                                  9   \n",
       "29                                                                  9   \n",
       "...                                                  ...          ...   \n",
       "27058  Chocolate, chocolate, chocolate!  This place d...           11   \n",
       "27059  Fresh fish. Great prices. Friendly staff. \\n\\n...            5   \n",
       "27060  Fresh fish. Great prices. Friendly staff. \\n\\n...           11   \n",
       "27061  Fresh fish. Great prices. Friendly staff. \\n\\n...            5   \n",
       "27062  Fresh fish. Great prices. Friendly staff. \\n\\n...            2   \n",
       "27063                                                               1   \n",
       "27064  Fresh fish. Great prices. Friendly staff. \\n\\n...           11   \n",
       "27065                                                               3   \n",
       "27066  Fresh fish. Great prices. Friendly staff. \\n\\n...            6   \n",
       "27067  Fresh fish. Great prices. Friendly staff. \\n\\n...            6   \n",
       "27068  Fresh fish. Great prices. Friendly staff. \\n\\n...            0   \n",
       "27069  Fresh fish. Great prices. Friendly staff. \\n\\n...            5   \n",
       "27070                                                               2   \n",
       "27071  Fresh fish. Great prices. Friendly staff. \\n\\n...            7   \n",
       "27072  Fresh fish. Great prices. Friendly staff. \\n\\n...            7   \n",
       "27073  Fresh fish. Great prices. Friendly staff. \\n\\n...            3   \n",
       "27074  Fresh fish. Great prices. Friendly staff. \\n\\n...           11   \n",
       "27075  Fresh fish. Great prices. Friendly staff. \\n\\n...            6   \n",
       "27076  Fresh fish. Great prices. Friendly staff. \\n\\n...            0   \n",
       "27077  So it was fate that I happened to be checking ...            3   \n",
       "27078  So it was fate that I happened to be checking ...            0   \n",
       "27079  So it was fate that I happened to be checking ...            1   \n",
       "27080  So it was fate that I happened to be checking ...            0   \n",
       "27081  So it was fate that I happened to be checking ...            2   \n",
       "27082  So it was fate that I happened to be checking ...            1   \n",
       "27083  So it was fate that I happened to be checking ...            1   \n",
       "27084  So it was fate that I happened to be checking ...            5   \n",
       "27085  Mama's Place might be our new favorite local p...            0   \n",
       "27086  Mama's Place might be our new favorite local p...            1   \n",
       "27087  Mama's Place might be our new favorite local p...            1   \n",
       "\n",
       "       score_lvl_2  score_lvl_3  \\\n",
       "0                0            0   \n",
       "1                0            1   \n",
       "2                0            0   \n",
       "3                0            1   \n",
       "4                0            0   \n",
       "5                0            0   \n",
       "6                0            1   \n",
       "7                0            0   \n",
       "8                0            1   \n",
       "9                0            0   \n",
       "10               0            1   \n",
       "11               0            0   \n",
       "12               0            1   \n",
       "13               0            1   \n",
       "14               0            0   \n",
       "15               0            1   \n",
       "16               0            4   \n",
       "17               0            3   \n",
       "18               0            2   \n",
       "19               0            3   \n",
       "20               0            0   \n",
       "21               0            4   \n",
       "22               0            3   \n",
       "23               0            0   \n",
       "24               0            0   \n",
       "25               0            1   \n",
       "26               0            2   \n",
       "27               2            2   \n",
       "28               0            2   \n",
       "29               0            2   \n",
       "...            ...          ...   \n",
       "27058            1            3   \n",
       "27059            0            2   \n",
       "27060            0            2   \n",
       "27061            0            1   \n",
       "27062            0            0   \n",
       "27063            0            1   \n",
       "27064            0            2   \n",
       "27065            0            1   \n",
       "27066            0            2   \n",
       "27067            0            2   \n",
       "27068            0            0   \n",
       "27069            0            1   \n",
       "27070            0            1   \n",
       "27071            0            1   \n",
       "27072            0            1   \n",
       "27073            0            0   \n",
       "27074            0            0   \n",
       "27075            0            2   \n",
       "27076            0            0   \n",
       "27077            0            0   \n",
       "27078            0            0   \n",
       "27079            0            1   \n",
       "27080            0            0   \n",
       "27081            1            0   \n",
       "27082            0            0   \n",
       "27083            0            1   \n",
       "27084            0            1   \n",
       "27085            0            0   \n",
       "27086            0            0   \n",
       "27087            0            0   \n",
       "\n",
       "                                preprocessed_review_text  \n",
       "0      this pretty typical cafe the sandwich wrap goo...  \n",
       "1      this place i like go deli sandwich financial d...  \n",
       "2      this place i like go deli sandwich financial d...  \n",
       "3      this place i like go deli sandwich financial d...  \n",
       "4      this place i like go deli sandwich financial d...  \n",
       "5      this place i like go deli sandwich financial d...  \n",
       "6      this place i like go deli sandwich financial d...  \n",
       "7      this place i like go deli sandwich financial d...  \n",
       "8      this place i like go deli sandwich financial d...  \n",
       "9      this place i like go deli sandwich financial d...  \n",
       "10     this place i like go deli sandwich financial d...  \n",
       "11     this place i like go deli sandwich financial d...  \n",
       "12     this place i like go deli sandwich financial d...  \n",
       "13     this place i like go deli sandwich financial d...  \n",
       "14                                                        \n",
       "15     considering i ve work upstairs place figure i ...  \n",
       "16     considering i ve work upstairs place figure i ...  \n",
       "17     considering i ve work upstairs place figure i ...  \n",
       "18     considering i ve work upstairs place figure i ...  \n",
       "19     considering i ve work upstairs place figure i ...  \n",
       "20     considering i ve work upstairs place figure i ...  \n",
       "21     considering i ve work upstairs place figure i ...  \n",
       "22     considering i ve work upstairs place figure i ...  \n",
       "23     considering i ve work upstairs place figure i ...  \n",
       "24     considering i ve work upstairs place figure i ...  \n",
       "25     considering i ve work upstairs place figure i ...  \n",
       "26                                                        \n",
       "27                                                        \n",
       "28                                                        \n",
       "29                                                        \n",
       "...                                                  ...  \n",
       "27058  chocolate chocolate chocolate this place cockt...  \n",
       "27059  fresh fish great price friendly staff i m exci...  \n",
       "27060  fresh fish great price friendly staff i m exci...  \n",
       "27061  fresh fish great price friendly staff i m exci...  \n",
       "27062  fresh fish great price friendly staff i m exci...  \n",
       "27063                                                     \n",
       "27064  fresh fish great price friendly staff i m exci...  \n",
       "27065                                                     \n",
       "27066  fresh fish great price friendly staff i m exci...  \n",
       "27067  fresh fish great price friendly staff i m exci...  \n",
       "27068  fresh fish great price friendly staff i m exci...  \n",
       "27069  fresh fish great price friendly staff i m exci...  \n",
       "27070                                                     \n",
       "27071  fresh fish great price friendly staff i m exci...  \n",
       "27072  fresh fish great price friendly staff i m exci...  \n",
       "27073  fresh fish great price friendly staff i m exci...  \n",
       "27074  fresh fish great price friendly staff i m exci...  \n",
       "27075  fresh fish great price friendly staff i m exci...  \n",
       "27076  fresh fish great price friendly staff i m exci...  \n",
       "27077  so fate i happen check twitter feed day find b...  \n",
       "27078  so fate i happen check twitter feed day find b...  \n",
       "27079  so fate i happen check twitter feed day find b...  \n",
       "27080  so fate i happen check twitter feed day find b...  \n",
       "27081  so fate i happen check twitter feed day find b...  \n",
       "27082  so fate i happen check twitter feed day find b...  \n",
       "27083  so fate i happen check twitter feed day find b...  \n",
       "27084  so fate i happen check twitter feed day find b...  \n",
       "27085  mama place might new favorite local place take...  \n",
       "27086  mama place might new favorite local place take...  \n",
       "27087  mama place might new favorite local place take...  \n",
       "\n",
       "[27088 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('pickle_jar/review_text_sentiment_hierarchical_df')\n",
    "# prep = pd.read_pickle('pickle_jar/preprocessed_review_text_hierarchical_df')\n",
    "# df = pd.concat([df, prep.preprocessed_review_text], axis=1)\n",
    "sim = pd.read_pickle('pickle_jar/similarity_vectors_df')\n",
    "# tfidf = joblib.load('pickle_jar/tfidf_preprocessed_ngram3_sublinear_1mil_hierarchical_dropna')\n",
    "# matrix = joblib.load('pickle_jar/similarity_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sim = sim[['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          [0.102025292814, 0.102025292814, 0.09906487166...\n",
       "1          [0.222249120474, 0.198464393616, 0.19058281183...\n",
       "2          [0.164701089263, 0.0990648716688, 0.0968299135...\n",
       "3          [0.0985388532281, 0.0823464468122, 0.071328066...\n",
       "4          [0.222249120474, 0.158419817686, 0.14401456713...\n",
       "5                                                        NaN\n",
       "6          [0.196376562119, 0.152856841683, 0.15220764279...\n",
       "7          [0.296981394291, 0.170809492469, 0.13077272474...\n",
       "8          [0.221467807889, 0.170809492469, 0.12393315881...\n",
       "9          [0.219487249851, 0.164701089263, 0.15220764279...\n",
       "10         [0.481546670198, 0.296981394291, 0.17592954635...\n",
       "11         [0.296981394291, 0.190560564399, 0.17080949246...\n",
       "12         [0.170227214694, 0.149701923132, 0.12393315881...\n",
       "13         [0.296981394291, 0.183491870761, 0.18349187076...\n",
       "14         [0.296981394291, 0.207707315683, 0.14350591599...\n",
       "15         [0.190560564399, 0.170809492469, 0.15279327333...\n",
       "16         [0.194799855351, 0.12238073349, 0.116296313703...\n",
       "17         [0.296981394291, 0.170809492469, 0.10959290713...\n",
       "18         [0.183491870761, 0.103699095547, 0.09853885322...\n",
       "19         [0.205153346062, 0.104428865016, 0.09234624356...\n",
       "20         [0.152207642794, 0.152207642794, 0.12075969576...\n",
       "21         [0.296981394291, 0.251105248928, 0.18673588335...\n",
       "22         [0.481546670198, 0.170809492469, 0.14350591599...\n",
       "23         [0.183491870761, 0.144014567137, 0.11629631370...\n",
       "24         [0.210461080074, 0.204275518656, 0.17970570921...\n",
       "25         [0.12521764636, 0.114993363619, 0.102025292814...\n",
       "26         [0.199039384723, 0.171422570944, 0.17142257094...\n",
       "27         [0.251105248928, 0.190919026732, 0.17080949246...\n",
       "28         [0.170809492469, 0.150684639812, 0.12393315881...\n",
       "29         [0.296981394291, 0.130772724748, 0.10450239479...\n",
       "                                 ...                        \n",
       "4071035                                                  NaN\n",
       "4071036                                                  NaN\n",
       "4071037                                                  NaN\n",
       "4071038                                                  NaN\n",
       "4071039                                                  NaN\n",
       "4071040                                                  NaN\n",
       "4071041                                                  NaN\n",
       "4071042                                                  NaN\n",
       "4071043                                                  NaN\n",
       "4071044                                                  NaN\n",
       "4071045                                                  NaN\n",
       "4071046                                                  NaN\n",
       "4071047                                                  NaN\n",
       "4071048                                                  NaN\n",
       "4071049                                                  NaN\n",
       "4071050                                                  NaN\n",
       "4071051                                                  NaN\n",
       "4071052                                                  NaN\n",
       "4071053                                                  NaN\n",
       "4071054                                                  NaN\n",
       "4071055                                                  NaN\n",
       "4071056                                                  NaN\n",
       "4071057    [0.0578195117414, 0.0382788032293, 0.005158562...\n",
       "4071058    [0.105976983905, 0.0985388532281, 0.0578195117...\n",
       "4071059    [0.0633905231953, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0...\n",
       "4071060                                                  NaN\n",
       "4071061                                                  NaN\n",
       "4071062                                                  NaN\n",
       "4071063                                                  NaN\n",
       "4071064    [0.104502394795, 0.0980260372162, 0.0923462435...\n",
       "Name: manager, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_out(x):\n",
    "    try:\n",
    "        return list(x)\n",
    "    except:\n",
    "        return x\n",
    "sim.manager.apply(get_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.previous_inspection_delta = df.previous_inspection_delta.fillna(0)\n",
    "df.previous_inspection_delta = df.previous_inspection_delta.dt.days.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (53 of 53) |#########################| Elapsed Time: 0:03:46 Time: 0:03:46\n"
     ]
    }
   ],
   "source": [
    "def get_out(x):\n",
    "    try:\n",
    "        return x[0]\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "topics = ['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "pbar = ProgressBar(maxval=len(topics)).start()\n",
    "for index, i in enumerate(topics):\n",
    "    df[i] = sim[i].apply(get_out)\n",
    "    pbar.update(index)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.concat([df,sim[topics]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data_grab.get_selects('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = df.dropna(subset=['review_text'])\n",
    "#1925254 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dropped['user_yelping_since_delta'] = (dropped.review_date - dropped.user_yelping_since).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dropped['user_most_recent_elite_year_delta'] = (dropped.review_date.dt.year - dropped.user_most_recent_elite_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "dropped['restaurant_categories'] = train.restaurant_categories.apply(lambda x: sorted(x))\n",
    "dropped['restaurant_neighborhoods'] = train.restaurant_neighborhoods.apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dropped.drop(['review_text', 'review_date', 'user_id', 'restaurant_full_address', 'restaurant_name',\n",
    "         'inspection_date', 'inspection_id', 'inspection_date', 'sentiment', 'vader'], axis=1, inplace=True)\n",
    "dropped.drop(['user_yelping_since', 'restaurant_attributes_by_appointment_only', 'restaurant_open', 'user_most_recent_elite_year'] , axis=1, inplace=True)\n",
    "dropped.drop(['review_year',\n",
    " 'review_month',\n",
    " 'review_day',\n",
    " 'review_dayofweek',\n",
    " 'review_quarter',\n",
    " 'review_dayofyear',\n",
    " 'inspection_dayofyear',], axis=1, inplace=True)\n",
    "# dropped.drop(['restaurant_neighborhoods', 'restaurant_categories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/frame.py:2148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "dropped['review_stars'] = dropped.review_stars.fillna(0).astype('category')\n",
    "dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer', 'checkin_counts']] = dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer', 'checkin_counts']].fillna(0)\n",
    "dropped[['restaurant_attributes_ages_allowed',\n",
    "         'restaurant_attributes_alcohol', \n",
    "         'restaurant_attributes_attire', \n",
    "         'restaurant_attributes_byob_corkage', \n",
    "         'restaurant_attributes_noise_level', \n",
    "         'restaurant_attributes_smoking', \n",
    "         'restaurant_attributes_wifi']] = dropped[['restaurant_attributes_ages_allowed',\n",
    "                                                   'restaurant_attributes_alcohol', \n",
    "                                                   'restaurant_attributes_attire', \n",
    "                                                   'restaurant_attributes_byob_corkage', \n",
    "                                                   'restaurant_attributes_noise_level', \n",
    "                                                   'restaurant_attributes_smoking', \n",
    "                                                   'restaurant_attributes_wifi']].convert_objects().fillna('nan')\n",
    "dropped[['restaurant_hours_friday_close',\n",
    " 'restaurant_hours_friday_open',\n",
    " 'restaurant_hours_monday_close',\n",
    " 'restaurant_hours_monday_open',\n",
    " 'restaurant_hours_saturday_close',\n",
    " 'restaurant_hours_saturday_open',\n",
    " 'restaurant_hours_sunday_close',\n",
    " 'restaurant_hours_sunday_open',\n",
    " 'restaurant_hours_thursday_close',\n",
    " 'restaurant_hours_thursday_open',\n",
    " 'restaurant_hours_tuesday_close',\n",
    " 'restaurant_hours_tuesday_open',\n",
    " 'restaurant_hours_wednesday_close',\n",
    " 'restaurant_hours_wednesday_open']] = dropped[[ 'restaurant_hours_friday_close',\n",
    " 'restaurant_hours_friday_open',\n",
    " 'restaurant_hours_monday_close',\n",
    " 'restaurant_hours_monday_open',\n",
    " 'restaurant_hours_saturday_close',\n",
    " 'restaurant_hours_saturday_open',\n",
    " 'restaurant_hours_sunday_close',\n",
    " 'restaurant_hours_sunday_open',\n",
    " 'restaurant_hours_thursday_close',\n",
    " 'restaurant_hours_thursday_open',\n",
    " 'restaurant_hours_tuesday_close',\n",
    " 'restaurant_hours_tuesday_open',\n",
    " 'restaurant_hours_wednesday_close',\n",
    " 'restaurant_hours_wednesday_open']].convert_objects().fillna('nan')\n",
    "dropped[['restaurant_ambience',\n",
    "         'restaurant_music',\n",
    "         'restaurant_parking',\n",
    "         'restaurant_zipcode']] = dropped[['restaurant_ambience',\n",
    "                                            'restaurant_music',\n",
    "                                            'restaurant_parking',\n",
    "                                            'restaurant_zipcode']].convert_objects().fillna('nan')\n",
    "dropped.user_most_recent_elite_year_delta = dropped.user_most_recent_elite_year_delta.fillna(dropped.user_most_recent_elite_year_delta.median())\n",
    "dropped.restaurant_attributes_price_range = dropped.restaurant_attributes_price_range.fillna(dropped.restaurant_attributes_price_range.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped[['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', \n",
    "         'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', \n",
    "         'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', \n",
    "         'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', \n",
    "         'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', \n",
    "         'pesticide', 'bugs', 'mold']] = dropped[['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', \n",
    "                                                  'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', \n",
    "                                                  'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', \n",
    "                                                  'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry',\n",
    "                                                  'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', \n",
    "                                                  'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old',\n",
    "                                                  'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', \n",
    "                                                  'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', \n",
    "                                                  'pesticide', 'bugs', 'mold']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = pd.read_pickle('pickle_jar/final_dropped')\n",
    "# dropped.to_pickle('pickle_jar/final_dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 177)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759039,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.user_most_recent_elite_year_delta.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3     1284912\n",
       "-2      143340\n",
       "-1      136361\n",
       "-4       98223\n",
       "-5       74546\n",
       "-6       56323\n",
       " 0       53969\n",
       "-7       40708\n",
       "-8       13722\n",
       " 1        9392\n",
       "-9        7155\n",
       " 2        3224\n",
       " 3        1631\n",
       "-10        953\n",
       " 4         541\n",
       " 5         149\n",
       " 6          51\n",
       "-11         32\n",
       " 7          22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.user_most_recent_elite_year_delta.value_counts(dropna=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for i in ['restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7']:\n",
    "    cats.extend(train[i].unique().tolist())\n",
    "cats = set(cats)\n",
    "cats.remove(np.nan)\n",
    "cats = sorted(cats)\n",
    "\n",
    "def proper_array(x, backfill_size=7):\n",
    "    encoder_prep = lambda x: cats.index(x)\n",
    "    temp = map(encoder_prep, x)\n",
    "    zeros = np.zeros(backfill_size, dtype='int')\n",
    "    zeros[:len(temp)] = temp\n",
    "    return zeros\n",
    "\n",
    "t = dropped.restaurant_categories.apply(proper_array)\n",
    "\n",
    "enc = OneHotEncoder(sparse=True)\n",
    "e = enc.fit_transform(np.vstack(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for i in ['restaurant_neighborhood_1', 'restaurant_neighborhood_2', 'restaurant_neighborhood_3']:\n",
    "    cats.extend(train[i].unique().tolist())\n",
    "cats = set(cats)\n",
    "cats.remove(np.nan)\n",
    "cats = sorted(cats)\n",
    "\n",
    "t = dropped.restaurant_neighborhoods.apply(proper_array, args=(3,))\n",
    "\n",
    "enc = OneHotEncoder(sparse=True)\n",
    "e = enc.fit_transform(np.vstack(t))\n",
    "\n",
    "def proper_array(x, backfill_size=3):\n",
    "    encoder_prep = lambda x: cats.index(x)\n",
    "    temp = map(encoder_prep, x)\n",
    "    zeros = np.zeros(backfill_size, dtype='int')\n",
    "    zeros[:len(temp)] = temp\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_label = LabelEncoder()\n",
    "# el = enc_label.fit_transform(np.vstack(t)[:,0])\n",
    "# el = enc_label.fit_transform(dropped.restaurant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_label.fit_transform(dropped.restaurant_attributes_accepts_credit_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelBinarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-08f32275db96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestaurant_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelBinarizer' is not defined"
     ]
    }
   ],
   "source": [
    "def add_categorical_to_matrix(matrix, df, columns):\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    for i in columns:\n",
    "        binarized = lb.fit_transform(df[i])\n",
    "        matrix = hstack([matrix, binarized])\n",
    "    return matrix\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "m = lb.fit_transform(dropped.restaurant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1925254x14 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1925254 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.restaurant_stars.dtypes\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "test = train.dropna(subset=['review_text']).restaurant_stars\n",
    "lb.fit_transform(np.array(test, dtype='|S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 82)\n",
      "(1925254, 95)\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame(dropped.restaurant_categories.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (95 of 95) |#########################| Elapsed Time: 0:02:32 Time: 0:02:32\n",
      "100% (87 of 87) |#########################| Elapsed Time: 0:01:38 Time: 0:01:38\n",
      "100% (46 of 46) |#########################| Elapsed Time: 0:00:20 Time: 0:00:20\n",
      "100% (29 of 29) |#########################| Elapsed Time: 0:00:23 Time: 0:00:23\n",
      "100% (15 of 15) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "100% (6 of 6) |###########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# need to do it like this because pd.merge causes a memory overload\n",
    "['restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7']:\n",
    "t0 = pd.get_dummies(temp[0])\n",
    "for i in range(1, 7):\n",
    "    new_dummies = pd.get_dummies(temp[i])\n",
    "    pbar = ProgressBar(maxval=len(new_dummies.columns)).start()\n",
    "    for index, column in enumerate(new_dummies.columns):\n",
    "        if column not in t0.columns:\n",
    "            t0 = pd.concat([t0, new_dummies[column]], axis=1)\n",
    "        else:\n",
    "            t0[column] = t0[column] + new_dummies[column]\n",
    "        pbar.update(index)\n",
    "    pbar.finish()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 158)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1925254x158 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 6416483 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dropped[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1923536x10881 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1923536 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = joblib.load('pickle_jar/similarity_matrix5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bins(df, bin_size=10):\n",
    "    # time delta bins\n",
    "    tdmax = df.review_delta.max()\n",
    "    tdmin = df.review_delta.min()\n",
    "    df['review_delta_bin'] = pd.cut(df[\"review_delta\"], np.arange(tdmin, tdmax, bin_size))\n",
    "    df['review_delta_bin_codes'] = df.review_delta_bin.astype('category').cat.codes\n",
    "    tdmax = df.previous_inspection_delta.max()\n",
    "    tdmin = df.previous_inspection_delta.min()\n",
    "    df['previous_inspection_delta_bin'] = pd.cut(df[\"previous_inspection_delta\"], np.arange(tdmin-1, tdmax, bin_size))\n",
    "    df['previous_inspection_delta_bin_codes'] = df.previous_inspection_delta_bin.astype('category').cat.codes\n",
    "    return df\n",
    "df = make_bins(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dropped[['review_votes_cool', 'review_votes_funny', 'review_votes_useful', 'user_average_stars', 'user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot', 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain', 'user_compliments_profile', 'user_compliments_writer', 'user_fans', 'user_review_count', 'user_votes_cool', 'user_votes_funny', 'user_votes_useful', 'restaurant_attributes_price_range', 'restaurant_latitude', 'restaurant_longitude', 'restaurant_review_count', 'checkin_counts', 'review_delta', 'previous_inspection_delta', 'polarity', 'subjectivity', 'neg', 'neu', 'pos', 'compound', 'user_yelping_since_delta','manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 177)\n",
      "(1925254, 87)\n",
      "(1925254, 3)\n"
     ]
    }
   ],
   "source": [
    "print dropped.shape\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 87)\n",
      "(1925254, 87)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def varthresh(X):\n",
    "    vt = VarianceThreshold()\n",
    "    new = vt.fit_transform(X)\n",
    "    print X.shape\n",
    "    print new.shape\n",
    "\n",
    "varthresh(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, f_classif, SelectFpr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def univariate(X, y, type):\n",
    "#     mms = MinMaxScaler()\n",
    "#     X_new = SelectKBest(chi2, k=2).fit_transform(mms.fit_transform(X), y)\n",
    "    kb = SelectKBest(type)\n",
    "    X_new = kb.fit_transform(X, y)\n",
    "    # index number of column sorted from most important to least important\n",
    "    ranking = np.argsort(kb.scores_)\n",
    "    print(\"Five least important features: \\n{}\".format('\\n'.join(X[ranking[:20]].columns)))\n",
    "    print(\"\\nFive most important features: \\n{}\".format('\\n'.join(X[ranking[:-21:-1]].columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_photos\n",
      "user_compliments_profile\n",
      "user_compliments_list\n",
      "user_compliments_more\n",
      "user_compliments_cute\n",
      "user_compliments_writer\n",
      "user_compliments_plain\n",
      "user_compliments_hot\n",
      "user_compliments_cool\n",
      "user_compliments_funny\n",
      "user_compliments_note\n",
      "user_fans\n",
      "user_votes_cool\n",
      "inspection\n",
      "user_votes_funny\n",
      "drip\n",
      "broken\n",
      "violation\n",
      "user_votes_useful\n",
      "hazard\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "checkin_counts\n",
      "restaurant_attributes_price_range\n",
      "restaurant_latitude\n",
      "restaurant_review_count\n",
      "restaurant_longitude\n",
      "review_delta\n",
      "sanitary\n",
      "hygiene\n",
      "user_yelping_since_delta\n",
      "undercooked\n",
      "lights\n",
      "cockroach\n",
      "parasite\n",
      "reheat\n",
      "contaminated\n",
      "dark\n",
      "frozen\n",
      "poisoning\n",
      "runny\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_1, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_photos\n",
      "puddle\n",
      "broken\n",
      "user_compliments_profile\n",
      "inspection\n",
      "illegal\n",
      "user_compliments_cute\n",
      "user_compliments_more\n",
      "wash\n",
      "sneeze\n",
      "trash\n",
      "bathroom\n",
      "user_compliments_list\n",
      "mouse\n",
      "user_compliments_plain\n",
      "user_compliments_funny\n",
      "user_compliments_note\n",
      "user_compliments_cool\n",
      "leak\n",
      "user_compliments_hot\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "restaurant_longitude\n",
      "restaurant_latitude\n",
      "checkin_counts\n",
      "restaurant_review_count\n",
      "sanitary\n",
      "hygiene\n",
      "lights\n",
      "restaurant_attributes_price_range\n",
      "cockroach\n",
      "dark\n",
      "review_delta\n",
      "contaminated\n",
      "undercooked\n",
      "poisoning\n",
      "parasite\n",
      "safety\n",
      "reheat\n",
      "pesticide\n",
      "rotten\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_2, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_list\n",
      "user_compliments_photos\n",
      "user_compliments_profile\n",
      "user_compliments_cute\n",
      "user_compliments_note\n",
      "user_compliments_funny\n",
      "user_fans\n",
      "hazard\n",
      "user_compliments_hot\n",
      "user_compliments_plain\n",
      "sneeze\n",
      "user_compliments_cool\n",
      "user_compliments_more\n",
      "violation\n",
      "inspection\n",
      "user_compliments_writer\n",
      "user_votes_funny\n",
      "disgusting\n",
      "user_votes_cool\n",
      "sick\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "restaurant_longitude\n",
      "restaurant_review_count\n",
      "restaurant_attributes_price_range\n",
      "checkin_counts\n",
      "restaurant_latitude\n",
      "cockroach\n",
      "undercooked\n",
      "review_delta\n",
      "bathroom\n",
      "old\n",
      "sanitary\n",
      "hygiene\n",
      "lights\n",
      "runny\n",
      "parasite\n",
      "reheat\n",
      "user_yelping_since_delta\n",
      "frozen\n",
      "toilet\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_3, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV, RFE\n",
    "\n",
    "def recurs(X, y):\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=10)\n",
    "    X_new = rfe.fit(X, y)\n",
    "    return X_new\n",
    "\n",
    "def recurscv(X, y):\n",
    "    rfecv = RFECV(estimator=SGDClassifier(n_jobs=-1), scoring='accuracy', cv=3)\n",
    "    X_new = rfecv.fit(X, y)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce dimensionality of data selecting for non-zero coefficients\n",
    "from sklearn.linear_model import RandomizedLogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svcbased(X, y):\n",
    "    # can remove features if they are closely correlated\n",
    "    svc = LinearSVC()\n",
    "    m = svc.fit_transform(X, y)\n",
    "    return m\n",
    "\n",
    "def rlrbased(X, y):\n",
    "    rlr = RandomizedLogisticRegression()\n",
    "    m = rlr.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def tree(X, y):\n",
    "    forest = ExtraTreesClassifier(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(10):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(10), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(10), indices)\n",
    "    plt.xlim([-1, 10])\n",
    "\n",
    "tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropped = pd.read_pickle('pickle_jar/final_dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = joblib.load('pickle_jar/final_matrix')\n",
    "# X = joblib.load('pickle_jar/specials_matrix')\n",
    "X = joblib.load('pickle_jar/categorical_matrix')\n",
    "y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = joblib.load( 'pickle_jar/tfidf_preprocessed_ngram3_sublinear_1mil_hierarchical_dropna')\n",
    "# y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 14088)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 1000000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sendMessage\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a990fb8c52e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msendMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoneTextSend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2)\n",
    "# lsa = TruncatedSVD(n_components=100)\n",
    "tfidf = lsa.fit_transform(tfidf)\n",
    "\n",
    "sendMessage.doneTextSend(t0, time(), 'LSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = hstack([tfidf, X])\n",
    "del tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-50b3bf8a0541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/lda.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, store_covariance, tol)\u001b[0m\n\u001b[1;32m    412\u001b[0m                           DeprecationWarning)\n\u001b[1;32m    413\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)\u001b[0m\n\u001b[1;32m    442\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    443\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     ensure_min_features)\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n\u001b[0;32m--> 334\u001b[0;31m                                       copy, force_all_finite)\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, order, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    240\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "ld = LDA()\n",
    "X = ld.fit_transform(X, y)\n",
    "\n",
    "\n",
    "sendMessage.doneTextSend(t0, time, 'LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 1014175)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.hstack((np.array(dropped[['review_delta', 'previous_inspection_delta']]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "import sendMessage\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "t0 = time()\n",
    "\n",
    "\n",
    "# set classifiers to test\n",
    "# estimator = LinearRegression()\n",
    "estimator = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = SGDClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = Perceptron(n_jobs=-1, random_state=42)  # gets some nuances\n",
    "# estimator = SGDRegressor() # gets some nuances\n",
    "# estimator = KNeighborsClassifier()\n",
    "# estimator = KNeighborsRegressor()  # gets some nuances\n",
    "# estimator = DecisionTreeClassifier()\n",
    "# estimator = DecisionTreeRegressor()\n",
    "# estimator = GaussianNB()\n",
    "# estimator = MultinomialNB()\n",
    "# estimator = LinearSVC(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#         ('zero_variance_removal', VarianceThreshold()),\n",
    "#         ('k_best', SelectKBest(score_func=f_classif, k=20)),\n",
    "#         ('no_negative', MinMaxScaler()),\n",
    "#         ('normalizer', Normalizer()),\n",
    "#         ('normalizer', Normalizer(norm='l2')), #  for text classification and clustering\n",
    "#         ('normalizer', Normalizer(copy=False)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('scaler', StandardScaler(with_mean=False)), #  for sparse matrix\n",
    "        ('clf', estimator),\n",
    "])\n",
    "\n",
    "p1,p2,p3,ytest = raw_fit(X, y, pipeline)\n",
    "raw_scoring(p1,p2,p3,ytest)\n",
    "\n",
    "\n",
    "print(\"{} seconds elapsed\".format(time()-t0))\n",
    "\n",
    "sendMessage.doneTextSend(t0, time(), 'just categorical matrix on rf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1: 0.22128560699\n",
      "level 2: 0.692799495547\n",
      "level 3: 0.573199692093\n"
     ]
    }
   ],
   "source": [
    "# baseline scores if guessing zero\n",
    "scores = ['score_lvl_1', 'score_lvl_2', 'score_lvl_3']\n",
    "guess = 0\n",
    "for index, score in enumerate(scores):\n",
    "    print(\"level {}: {}\".format(index+1, y[score].value_counts(normalize=True)[guess]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 accuracy score of 0.22128560699\n",
      "Level 2 accuracy score of 0.692799495547\n",
      "Level 3 accuracy score of 0.573199692093\n",
      "Contest score of 2.14265412097\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22128560699003871,\n",
       " 0.69279949554708109,\n",
       " 0.57319969209257582,\n",
       " 2.1426541209656755,\n",
       "    pred1  pred2  pred3  round1  round2  round3  true1  true2  true3  offset1  \\\n",
       " 0      0      0      0       0       0       0      2      0      0       -2   \n",
       " 1      0      0      0       0       0       0      2      0      0       -2   \n",
       " 2      0      0      0       0       0       0      2      0      0       -2   \n",
       " 3      0      0      0       0       0       0      2      0      0       -2   \n",
       " 4      0      0      0       0       0       0      2      0      0       -2   \n",
       " 5      0      0      0       0       0       0      7      0      1       -7   \n",
       " 6      0      0      0       0       0       0      7      0      1       -7   \n",
       " 7      0      0      0       0       0       0      7      0      1       -7   \n",
       " 8      0      0      0       0       0       0      7      0      1       -7   \n",
       " 9      0      0      0       0       0       0      7      0      1       -7   \n",
       " \n",
       "    offset2  offset3  \n",
       " 0        0        0  \n",
       " 1        0        0  \n",
       " 2        0        0  \n",
       " 3        0        0  \n",
       " 4        0        0  \n",
       " 5        0       -1  \n",
       " 6        0       -1  \n",
       " 7        0       -1  \n",
       " 8        0       -1  \n",
       " 9        0       -1  )"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = np.zeros(1925254)\n",
    "raw_scoring(p, p, p, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
