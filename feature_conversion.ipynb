{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import text_processors\n",
    "from progressbar import ProgressBar\n",
    "import data_grab\n",
    "from time import time\n",
    "from sklearn.externals import joblib\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "from sklearn.feature_selection import VarianceThreshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_metric(numpy_array_predictions, numpy_array_actual_values):\n",
    "    return metrics.weighted_rmsle(numpy_array_predictions, numpy_array_actual_values,\n",
    "            weights=metrics.KEEPING_IT_CLEAN_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def contest_scoring(X, y, pipeline):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "    s1 = pipeline.fit(X_train, y_train['score_lvl_1']).predict(X_test)\n",
    "    s2 = pipeline.fit(X_train, y_train['score_lvl_2']).predict(X_test)\n",
    "    s3 = pipeline.fit(X_train, y_train['score_lvl_3']).predict(X_test)\n",
    "    results = np.dstack((s1, s2, s3))\n",
    "    score = contest_metric(np.round(results[0]), np.array(y_test))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def raw_scoring(p1, p2, p3, ytrue):\n",
    "    '''since cross_val_score doesn't allow you to round the results beforehand. also for pymc3 and other non-sklearn models'''\n",
    "    score1 = accuracy_score(ytrue['score_lvl_1'], np.clip(np.round(p1), 0, np.inf))\n",
    "    print(\"Level 1 accuracy score of {}\".format(score1))\n",
    "    score2 = accuracy_score(ytrue['score_lvl_2'],np.clip(np.round(p2), 0, np.inf))\n",
    "    print(\"Level 2 accuracy score of {}\".format(score2))\n",
    "    score3 = accuracy_score(ytrue['score_lvl_3'], np.clip(np.round(p3), 0, np.inf))\n",
    "    print(\"Level 3 accuracy score of {}\".format(score3))\n",
    "    \n",
    "    results = np.dstack((p1, p2, p3))[0]\n",
    "    rounded = np.clip(np.round(results), 0, np.inf)\n",
    "    score = contest_metric(rounded, np.array(ytrue))\n",
    "    print(\"Contest score of {}\".format(score))\n",
    "    \n",
    "    compare = pd.concat([pd.DataFrame(np.concatenate((results, rounded), axis=1)), ytrue.reset_index(drop=True)], axis=1)\n",
    "    compare.columns = ['pred1','pred2','pred3','round1','round2','round3','true1','true2','true3']\n",
    "    compare['offset1'] = compare.round1-compare.true1\n",
    "    compare['offset2'] = compare.round2-compare.true2\n",
    "    compare['offset3'] = compare.round3-compare.true3\n",
    "        \n",
    "    return score1, score2, score3, score, compare.head(10)\n",
    "\n",
    "    \n",
    "def raw_fit(X, y, pipeline):\n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(X, y, random_state=42)\n",
    "    \n",
    "    p1 = pipeline.fit(xtrain, ytrain['score_lvl_1']).predict(xtest)\n",
    "    p2 = pipeline.fit(xtrain, ytrain['score_lvl_2']).predict(xtest)\n",
    "    p3 = pipeline.fit(xtrain, ytrain['score_lvl_3']).predict(xtest)\n",
    "        \n",
    "    return p1, p2, p3, ytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    features = df.drop(['score_lvl_1', 'score_lvl_2', 'score_lvl_3'], axis=1)\n",
    "    response = df[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']].astype(np.int8)\n",
    "    \n",
    "    return features, response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_pickle('pickle_jar/review_text_sentiment_hierarchical_df')\n",
    "# prep = pd.read_pickle('pickle_jar/preprocessed_review_text_hierarchical_df')\n",
    "# df = pd.concat([df, prep.preprocessed_review_text], axis=1)\n",
    "sim = pd.read_pickle('pickle_jar/similarity_vectors_df')\n",
    "# tfidf = joblib.load('pickle_jar/tfidf_preprocessed_ngram3_sublinear_1mil_hierarchical_dropna')\n",
    "# matrix = joblib.load('pickle_jar/similarity_matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.previous_inspection_delta = df.previous_inspection_delta.fillna(0)\n",
    "df.previous_inspection_delta = df.previous_inspection_delta.dt.days.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (53 of 53) |#########################| Elapsed Time: 0:03:46 Time: 0:03:46\n"
     ]
    }
   ],
   "source": [
    "def get_out(x):\n",
    "    try:\n",
    "        return x[0]\n",
    "    except:\n",
    "        return x\n",
    "\n",
    "\n",
    "topics = ['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold', ]\n",
    "pbar = ProgressBar(maxval=len(topics)).start()\n",
    "for index, i in enumerate(topics):\n",
    "    df[i] = sim[i].apply(get_out)\n",
    "    pbar.update(index)\n",
    "pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df = pd.concat([df,sim[topics]],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = data_grab.get_selects('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = df.dropna(subset=['review_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dropped['user_yelping_since_delta'] = (dropped.review_date - dropped.user_yelping_since).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "dropped['user_most_recent_elite_year_delta'] = (dropped.review_date.dt.year - dropped.user_most_recent_elite_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "dropped['restaurant_categories'] = train.restaurant_categories.apply(lambda x: sorted(x))\n",
    "dropped['restaurant_neighborhoods'] = train.restaurant_neighborhoods.apply(lambda x: sorted(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from IPython.kernel.zmq import kernelapp as app\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "dropped.drop(['review_text', 'review_date', 'user_id', 'restaurant_full_address', 'restaurant_name',\n",
    "         'inspection_date', 'inspection_id', 'inspection_date', 'sentiment', 'vader'], axis=1, inplace=True)\n",
    "dropped.drop(['user_yelping_since', 'restaurant_attributes_by_appointment_only', 'restaurant_open', 'user_most_recent_elite_year'] , axis=1, inplace=True)\n",
    "dropped.drop(['review_year',\n",
    " 'review_month',\n",
    " 'review_day',\n",
    " 'review_dayofweek',\n",
    " 'review_quarter',\n",
    " 'review_dayofyear',\n",
    " 'inspection_dayofyear',], axis=1, inplace=True)\n",
    "# dropped.drop(['restaurant_neighborhoods', 'restaurant_categories'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/IPython/kernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/frame.py:2148: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[k1] = value[k2]\n",
      "/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/pandas/core/generic.py:2177: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "dropped['review_stars'] = dropped.review_stars.fillna(0).astype('category')\n",
    "dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer', 'checkin_counts']] = dropped[['user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot',\n",
    " 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain',\n",
    " 'user_compliments_profile', 'user_compliments_writer', 'checkin_counts']].fillna(0)\n",
    "dropped[['restaurant_attributes_ages_allowed',\n",
    "         'restaurant_attributes_alcohol', \n",
    "         'restaurant_attributes_attire', \n",
    "         'restaurant_attributes_byob_corkage', \n",
    "         'restaurant_attributes_noise_level', \n",
    "         'restaurant_attributes_smoking', \n",
    "         'restaurant_attributes_wifi']] = dropped[['restaurant_attributes_ages_allowed',\n",
    "                                                   'restaurant_attributes_alcohol', \n",
    "                                                   'restaurant_attributes_attire', \n",
    "                                                   'restaurant_attributes_byob_corkage', \n",
    "                                                   'restaurant_attributes_noise_level', \n",
    "                                                   'restaurant_attributes_smoking', \n",
    "                                                   'restaurant_attributes_wifi']].convert_objects().fillna('nan')\n",
    "dropped[['restaurant_hours_friday_close',\n",
    " 'restaurant_hours_friday_open',\n",
    " 'restaurant_hours_monday_close',\n",
    " 'restaurant_hours_monday_open',\n",
    " 'restaurant_hours_saturday_close',\n",
    " 'restaurant_hours_saturday_open',\n",
    " 'restaurant_hours_sunday_close',\n",
    " 'restaurant_hours_sunday_open',\n",
    " 'restaurant_hours_thursday_close',\n",
    " 'restaurant_hours_thursday_open',\n",
    " 'restaurant_hours_tuesday_close',\n",
    " 'restaurant_hours_tuesday_open',\n",
    " 'restaurant_hours_wednesday_close',\n",
    " 'restaurant_hours_wednesday_open']] = dropped[[ 'restaurant_hours_friday_close',\n",
    " 'restaurant_hours_friday_open',\n",
    " 'restaurant_hours_monday_close',\n",
    " 'restaurant_hours_monday_open',\n",
    " 'restaurant_hours_saturday_close',\n",
    " 'restaurant_hours_saturday_open',\n",
    " 'restaurant_hours_sunday_close',\n",
    " 'restaurant_hours_sunday_open',\n",
    " 'restaurant_hours_thursday_close',\n",
    " 'restaurant_hours_thursday_open',\n",
    " 'restaurant_hours_tuesday_close',\n",
    " 'restaurant_hours_tuesday_open',\n",
    " 'restaurant_hours_wednesday_close',\n",
    " 'restaurant_hours_wednesday_open']].convert_objects().fillna('nan')\n",
    "dropped[['restaurant_ambience',\n",
    "         'restaurant_music',\n",
    "         'restaurant_parking',\n",
    "         'restaurant_zipcode']] = dropped[['restaurant_ambience',\n",
    "                                            'restaurant_music',\n",
    "                                            'restaurant_parking',\n",
    "                                            'restaurant_zipcode']].convert_objects().fillna('nan')\n",
    "dropped.user_most_recent_elite_year_delta = dropped.user_most_recent_elite_year_delta.fillna(dropped.user_most_recent_elite_year_delta.median())\n",
    "dropped.restaurant_attributes_price_range = dropped.restaurant_attributes_price_range.fillna(dropped.restaurant_attributes_price_range.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped[['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', \n",
    "         'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', \n",
    "         'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', \n",
    "         'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', \n",
    "         'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', \n",
    "         'pesticide', 'bugs', 'mold']] = dropped[['manager', 'supervisor', 'training', 'safety', 'disease', 'ill', \n",
    "                                                  'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', \n",
    "                                                  'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', \n",
    "                                                  'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry',\n",
    "                                                  'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', \n",
    "                                                  'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old',\n",
    "                                                  'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', \n",
    "                                                  'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', \n",
    "                                                  'pesticide', 'bugs', 'mold']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropped = pd.read_pickle('pickle_jar/final_dropped')\n",
    "# dropped.to_pickle('pickle_jar/final_dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 177)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del df\n",
    "del train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(759039,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.user_most_recent_elite_year_delta.dropna().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3     1284912\n",
       "-2      143340\n",
       "-1      136361\n",
       "-4       98223\n",
       "-5       74546\n",
       "-6       56323\n",
       " 0       53969\n",
       "-7       40708\n",
       "-8       13722\n",
       " 1        9392\n",
       "-9        7155\n",
       " 2        3224\n",
       " 3        1631\n",
       "-10        953\n",
       " 4         541\n",
       " 5         149\n",
       " 6          51\n",
       "-11         32\n",
       " 7          22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.user_most_recent_elite_year_delta.value_counts(dropna=False, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for i in ['restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7']:\n",
    "    cats.extend(train[i].unique().tolist())\n",
    "cats = set(cats)\n",
    "cats.remove(np.nan)\n",
    "cats = sorted(cats)\n",
    "\n",
    "def proper_array(x, backfill_size=7):\n",
    "    encoder_prep = lambda x: cats.index(x)\n",
    "    temp = map(encoder_prep, x)\n",
    "    zeros = np.zeros(backfill_size, dtype='int')\n",
    "    zeros[:len(temp)] = temp\n",
    "    return zeros\n",
    "\n",
    "t = dropped.restaurant_categories.apply(proper_array)\n",
    "\n",
    "enc = OneHotEncoder(sparse=True)\n",
    "e = enc.fit_transform(np.vstack(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cats = []\n",
    "for i in ['restaurant_neighborhood_1', 'restaurant_neighborhood_2', 'restaurant_neighborhood_3']:\n",
    "    cats.extend(train[i].unique().tolist())\n",
    "cats = set(cats)\n",
    "cats.remove(np.nan)\n",
    "cats = sorted(cats)\n",
    "\n",
    "t = dropped.restaurant_neighborhoods.apply(proper_array, args=(3,))\n",
    "\n",
    "enc = OneHotEncoder(sparse=True)\n",
    "e = enc.fit_transform(np.vstack(t))\n",
    "\n",
    "def proper_array(x, backfill_size=3):\n",
    "    encoder_prep = lambda x: cats.index(x)\n",
    "    temp = map(encoder_prep, x)\n",
    "    zeros = np.zeros(backfill_size, dtype='int')\n",
    "    zeros[:len(temp)] = temp\n",
    "    return temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_label = LabelEncoder()\n",
    "# el = enc_label.fit_transform(np.vstack(t)[:,0])\n",
    "# el = enc_label.fit_transform(dropped.restaurant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_label.fit_transform(dropped.restaurant_attributes_accepts_credit_cards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelBinarizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-08f32275db96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mlb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelBinarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestaurant_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LabelBinarizer' is not defined"
     ]
    }
   ],
   "source": [
    "def add_categorical_to_matrix(matrix, df, columns):\n",
    "    lb = LabelBinarizer(sparse_output=True)\n",
    "    for i in columns:\n",
    "        binarized = lb.fit_transform(df[i])\n",
    "        matrix = hstack([matrix, binarized])\n",
    "    return matrix\n",
    "\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "m = lb.fit_transform(dropped.restaurant_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1925254x14 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1925254 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropped.restaurant_stars.dtypes\n",
    "lb = LabelBinarizer(sparse_output=True)\n",
    "test = train.dropna(subset=['review_text']).restaurant_stars\n",
    "lb.fit_transform(np.array(test, dtype='|S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 82)\n",
      "(1925254, 95)\n"
     ]
    }
   ],
   "source": [
    "temp = pd.DataFrame(dropped.restaurant_categories.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100% (95 of 95) |#########################| Elapsed Time: 0:02:32 Time: 0:02:32\n",
      "100% (87 of 87) |#########################| Elapsed Time: 0:01:38 Time: 0:01:38\n",
      "100% (46 of 46) |#########################| Elapsed Time: 0:00:20 Time: 0:00:20\n",
      "100% (29 of 29) |#########################| Elapsed Time: 0:00:23 Time: 0:00:23\n",
      "100% (15 of 15) |#########################| Elapsed Time: 0:00:00 Time: 0:00:00\n",
      "100% (6 of 6) |###########################| Elapsed Time: 0:00:00 Time: 0:00:00\n"
     ]
    }
   ],
   "source": [
    "# need to do it like this because pd.merge causes a memory overload\n",
    "['restaurant_category_1',\n",
    " 'restaurant_category_2',\n",
    " 'restaurant_category_3',\n",
    " 'restaurant_category_4',\n",
    " 'restaurant_category_5',\n",
    " 'restaurant_category_6',\n",
    " 'restaurant_category_7']:\n",
    "t0 = pd.get_dummies(temp[0])\n",
    "for i in range(1, 7):\n",
    "    new_dummies = pd.get_dummies(temp[i])\n",
    "    pbar = ProgressBar(maxval=len(new_dummies.columns)).start()\n",
    "    for index, column in enumerate(new_dummies.columns):\n",
    "        if column not in t0.columns:\n",
    "            t0 = pd.concat([t0, new_dummies[column]], axis=1)\n",
    "        else:\n",
    "            t0[column] = t0[column] + new_dummies[column]\n",
    "        pbar.update(index)\n",
    "    pbar.finish()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 158)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1925254x158 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 6416483 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr_matrix(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = dropped[['score_lvl_1', 'score_lvl_2', 'score_lvl_3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1923536x10881 sparse matrix of type '<type 'numpy.int64'>'\n",
       "\twith 1923536 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matrix = joblib.load('pickle_jar/similarity_matrix5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bins(df, bin_size=10):\n",
    "    # time delta bins\n",
    "    tdmax = df.review_delta.max()\n",
    "    tdmin = df.review_delta.min()\n",
    "    df['review_delta_bin'] = pd.cut(df[\"review_delta\"], np.arange(tdmin, tdmax, bin_size))\n",
    "    df['review_delta_bin_codes'] = df.review_delta_bin.astype('category').cat.codes\n",
    "    tdmax = df.previous_inspection_delta.max()\n",
    "    tdmin = df.previous_inspection_delta.min()\n",
    "    df['previous_inspection_delta_bin'] = pd.cut(df[\"previous_inspection_delta\"], np.arange(tdmin-1, tdmax, bin_size))\n",
    "    df['previous_inspection_delta_bin_codes'] = df.previous_inspection_delta_bin.astype('category').cat.codes\n",
    "    return df\n",
    "df = make_bins(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = dropped[['review_votes_cool', 'review_votes_funny', 'review_votes_useful', 'user_average_stars', 'user_compliments_cool', 'user_compliments_cute', 'user_compliments_funny', 'user_compliments_hot', 'user_compliments_list', 'user_compliments_more', 'user_compliments_note', 'user_compliments_photos', 'user_compliments_plain', 'user_compliments_profile', 'user_compliments_writer', 'user_fans', 'user_review_count', 'user_votes_cool', 'user_votes_funny', 'user_votes_useful', 'restaurant_attributes_price_range', 'restaurant_latitude', 'restaurant_longitude', 'restaurant_review_count', 'checkin_counts', 'review_delta', 'previous_inspection_delta', 'polarity', 'subjectivity', 'neg', 'neu', 'pos', 'compound', 'user_yelping_since_delta','manager', 'supervisor', 'training', 'safety', 'disease', 'ill', 'sick', 'poisoning', 'hygiene', 'raw', 'undercooked', 'cold', 'clean', 'sanitary', 'wash', 'jaundice', 'yellow', 'hazard', 'inspection', 'violation', 'gloves', 'hairnet', 'nails', 'jewelry', 'sneeze', 'cough', 'runny', 'illegal', 'rotten', 'dirty', 'mouse', 'cockroach', 'contaminated', 'gross', 'disgusting', 'stink', 'old', 'parasite', 'reheat', 'frozen', 'broken', 'drip', 'bathroom', 'toilet', 'leak', 'trash', 'dark', 'lights', 'dust', 'puddle', 'pesticide', 'bugs', 'mold']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 177)\n",
      "(1925254, 87)\n",
      "(1925254, 3)\n"
     ]
    }
   ],
   "source": [
    "print dropped.shape\n",
    "print X.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1925254, 87)\n",
      "(1925254, 87)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def varthresh(X):\n",
    "    vt = VarianceThreshold()\n",
    "    new = vt.fit_transform(X)\n",
    "    print X.shape\n",
    "    print new.shape\n",
    "\n",
    "varthresh(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2, f_regression, f_classif, SelectFpr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def univariate(X, y, type):\n",
    "#     mms = MinMaxScaler()\n",
    "#     X_new = SelectKBest(chi2, k=2).fit_transform(mms.fit_transform(X), y)\n",
    "    kb = SelectKBest(type)\n",
    "    X_new = kb.fit_transform(X, y)\n",
    "    # index number of column sorted from most important to least important\n",
    "    ranking = np.argsort(kb.scores_)\n",
    "    print(\"Five least important features: \\n{}\".format('\\n'.join(X[ranking[:10]].columns)))\n",
    "    print(\"\\nFive most important features: \\n{}\".format('\\n'.join(X[ranking[:-11:-1]].columns)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_photos\n",
      "user_compliments_profile\n",
      "user_compliments_list\n",
      "user_compliments_more\n",
      "user_compliments_cute\n",
      "user_compliments_writer\n",
      "user_compliments_plain\n",
      "user_compliments_hot\n",
      "user_compliments_cool\n",
      "user_compliments_funny\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "checkin_counts\n",
      "restaurant_attributes_price_range\n",
      "restaurant_latitude\n",
      "restaurant_review_count\n",
      "restaurant_longitude\n",
      "review_delta\n",
      "sanitary\n",
      "hygiene\n",
      "user_yelping_since_delta\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_1, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_photos\n",
      "puddle\n",
      "broken\n",
      "user_compliments_profile\n",
      "inspection\n",
      "illegal\n",
      "user_compliments_cute\n",
      "user_compliments_more\n",
      "wash\n",
      "sneeze\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "restaurant_longitude\n",
      "restaurant_latitude\n",
      "checkin_counts\n",
      "restaurant_review_count\n",
      "sanitary\n",
      "hygiene\n",
      "lights\n",
      "restaurant_attributes_price_range\n",
      "cockroach\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_2, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Five least important features: \n",
      "user_compliments_list\n",
      "user_compliments_photos\n",
      "user_compliments_profile\n",
      "user_compliments_cute\n",
      "user_compliments_note\n",
      "user_compliments_funny\n",
      "user_fans\n",
      "hazard\n",
      "user_compliments_hot\n",
      "user_compliments_plain\n",
      "\n",
      "Five most important features: \n",
      "previous_inspection_delta\n",
      "restaurant_longitude\n",
      "restaurant_review_count\n",
      "restaurant_attributes_price_range\n",
      "checkin_counts\n",
      "restaurant_latitude\n",
      "cockroach\n",
      "undercooked\n",
      "review_delta\n",
      "bathroom\n"
     ]
    }
   ],
   "source": [
    "X_new = univariate(X, y.score_lvl_3, f_classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV, RFE\n",
    "\n",
    "def recurs(X, y):\n",
    "    rfe = RFE(estimator=LinearRegression(), n_features_to_select=10)\n",
    "    X_new = rfe.fit(X, y)\n",
    "    return X_new\n",
    "\n",
    "def recurscv(X, y):\n",
    "    rfecv = RFECV(estimator=SGDClassifier(n_jobs=-1), scoring='accuracy', cv=3)\n",
    "    X_new = rfecv.fit(X, y)\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reduce dimensionality of data selecting for non-zero coefficients\n",
    "from sklearn.linear_model import RandomizedLogisticRegression \n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def svcbased(X, y):\n",
    "    # can remove features if they are closely correlated\n",
    "    svc = LinearSVC()\n",
    "    m = svc.fit_transform(X, y)\n",
    "    return m\n",
    "\n",
    "def rlrbased(X, y):\n",
    "    rlr = RandomizedLogisticRegression()\n",
    "    m = rlr.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "def tree(X, y):\n",
    "    forest = ExtraTreesClassifier(n_estimators=10, random_state=42, n_jobs=-1)\n",
    "    forest.fit(X, y)\n",
    "    importances = forest.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                 axis=0)\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Print the feature ranking\n",
    "    print(\"Feature ranking:\")\n",
    "\n",
    "    for f in range(10):\n",
    "        print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "    # Plot the feature importances of the forest\n",
    "    plt.figure()\n",
    "    plt.title(\"Feature importances\")\n",
    "    plt.bar(range(10), importances[indices],\n",
    "           color=\"r\", yerr=std[indices], align=\"center\")\n",
    "    plt.xticks(range(10), indices)\n",
    "    plt.xlim([-1, 10])\n",
    "\n",
    "tree(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropped = pd.read_pickle('pickle_jar/final_dropped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = joblib.load('pickle_jar/final_matrix')\n",
    "y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf = joblib.load( 'pickle_jar/tfidf_preprocessed_ngram3_sublinear_1mil_hierarchical_dropna')\n",
    "# y = joblib.load('pickle_jar/final_y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 14175)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 1000000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sendMessage\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-a990fb8c52e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlsa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msendMessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoneTextSend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'LSA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 't1' is not defined"
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "lsa = TruncatedSVD(n_components=2)\n",
    "# lsa = TruncatedSVD(n_components=100)\n",
    "tfidf = lsa.fit_transform(tfidf)\n",
    "\n",
    "sendMessage.doneTextSend(t0, time(), 'LSA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = hstack([tfidf, X])\n",
    "del tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-50b3bf8a0541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mld\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/base.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/lda.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, store_covariance, tol)\u001b[0m\n\u001b[1;32m    412\u001b[0m                           DeprecationWarning)\n\u001b[1;32m    413\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric)\u001b[0m\n\u001b[1;32m    442\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    443\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m                     ensure_min_features)\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         array = _ensure_sparse_format(array, accept_sparse, dtype, order,\n\u001b[0;32m--> 334\u001b[0;31m                                       copy, force_all_finite)\n\u001b[0m\u001b[1;32m    335\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/amangum/anaconda/envs/datasci/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36m_ensure_sparse_format\u001b[0;34m(spmatrix, accept_sparse, dtype, order, copy, force_all_finite)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \"\"\"\n\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maccept_sparse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         raise TypeError('A sparse matrix was passed, but dense '\n\u001b[0m\u001b[1;32m    240\u001b[0m                         \u001b[0;34m'data is required. Use X.toarray() to '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                         'convert to a dense numpy array.')\n",
      "\u001b[0;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "t0=time()\n",
    "from sklearn.lda import LDA\n",
    "\n",
    "ld = LDA()\n",
    "X = ld.fit_transform(X, y)\n",
    "\n",
    "\n",
    "sendMessage.doneTextSend(t0, time, 'LDA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925254, 1014175)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.hstack((np.array(dropped[['review_delta', 'previous_inspection_delta']]), X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "\n",
    "\n",
    "# set classifiers to test\n",
    "# estimator = LinearRegression()\n",
    "estimator = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = SGDClassifier(n_jobs=-1, random_state=42)\n",
    "# estimator = Perceptron(n_jobs=-1, random_state=42)  # gets some nuances\n",
    "# estimator = SGDRegressor() # gets some nuances\n",
    "# estimator = KNeighborsClassifier()\n",
    "# estimator = KNeighborsRegressor()  # gets some nuances\n",
    "# estimator = DecisionTreeClassifier()\n",
    "# estimator = DecisionTreeRegressor()\n",
    "# estimator = GaussianNB()\n",
    "# estimator = MultinomialNB()\n",
    "# estimator = LinearSVC(random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('zero_variance_removal', VarianceThreshold()),\n",
    "#         ('no_negative', MinMaxScaler()),\n",
    "#         ('normalizer', Normalizer()),\n",
    "#         ('normalizer', Normalizer(norm='l2')), #  for text classification and clustering\n",
    "#         ('normalizer', Normalizer(copy=False)),\n",
    "#         ('scaler', StandardScaler()),\n",
    "#         ('scaler', StandardScaler(with_mean=False)), #  for sparse matrix\n",
    "        ('clf', estimator),\n",
    "])\n",
    "\n",
    "p1,p2,p3,ytest = raw_fit(X, y, pipeline)\n",
    "raw_scoring(p1,p2,p3,ytest)\n",
    "\n",
    "\n",
    "print(\"{} seconds elapsed\".format(time()-t0))\n",
    "sendMessage.doneTextSend(t0, time(), 'tfidf + matrix, no lsa')\n",
    "\n",
    "# first representation of manager and mold plus deltas for SGDClassifier\n",
    "# Level 1 accuracy score of 0.160254863959\n",
    "# Level 2 accuracy score of 0.693050714933\n",
    "# Level 3 accuracy score of 0.574146779681\n",
    "# Contest score of 1.52839121903\n",
    "\n",
    "# sparse matrix of just manager no-mean-false and the detla for sdg\n",
    "# Level 1 accuracy score of 0.136340572778\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.41908388602\n",
    "\n",
    "# first representation of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.0958193660009\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.2018434011\n",
    "\n",
    "# first two representations of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.134818376157\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.73133292154\n",
    "\n",
    "# first five representations of all similarity vecs plus deltas for SGD\n",
    "# Level 1 accuracy score of 0.140464228379\n",
    "# Level 2 accuracy score of 0.693059032948\n",
    "# Level 3 accuracy score of 0.574184210745\n",
    "# Contest score of 1.43479163855"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 1: 0.22128560699\n",
      "level 2: 0.692799495547\n",
      "level 3: 0.573199692093\n"
     ]
    }
   ],
   "source": [
    "# baseline scores if guessing zero\n",
    "scores = ['score_lvl_1', 'score_lvl_2', 'score_lvl_3']\n",
    "guess = 0\n",
    "for index, score in enumerate(scores):\n",
    "    print(\"level {}: {}\".format(index+1, y[score].value_counts(normalize=True)[guess]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
